{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 知识工程-问答系统设计与实现大作业\n",
    "2024214500 叶璨铭\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 代码与文档格式说明\n",
    "\n",
    "> 本文档使用Jupyter Notebook编写，遵循Diátaxis 系统 Notebook实践 https://nbdev.fast.ai/tutorials/best_practices.html，所以同时包括了实验文档和实验代码。\n",
    "\n",
    "> 本文档理论上支持多个格式，包括ipynb, docx, pdf 等。您在阅读本文档时，可以选择您喜欢的格式来进行阅读，建议您使用 Visual Studio Code (或者其他支持jupyter notebook的IDE, 但是VSCode阅读体验最佳) 打开 `ipynb`格式的文档来进行阅读。\n",
    "\n",
    "> 为了记录我们自己修改了哪些地方，使用git进行版本控制，这样可以清晰地看出我们基于助教的代码在哪些位置进行了修改，有些修改是实现了要求的作业功能，而有些代码是对原本代码进行了重构和优化。我将我在知识工程课程的代码，在作业截止DDL之后，开源到 https://github.com/2catycm/THU-Coursework-Knowledge-Engineering.git ，方便各位同学一起学习讨论。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9fd40",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "\n",
    "非常好的助教已经帮我们down好了数据，注意到有四个文件\n",
    "- passages_multi_sentences.json (文档库) \n",
    "- train.json (问答语料) \n",
    "- train_questions.txt (问题分类训练语料) \n",
    "- test_questions.txt (问题分类测试语料) \n",
    "\n",
    "我们自己查看一下数据格式\n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "![alt text](image-1.png)\n",
    "\n",
    "\n",
    "| 文件名 (File Name)             | 来源数据集 (Source Dataset)                                  | 数据集描述 (Dataset Description)                                                                                                                               | 文件格式 (File Format)                                                                                                                                  |\n",
    "| :----------------------------- | :----------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `passages_multi_sentences.json` | 百度 DuReader V2.0 (筛选处理后)                     | 包含用于检索和答案抽取的文档库，这些文档被分割成了多个句子 。                                                                                             | 实际上是JSONL，每一行是JSON 格式。每一个json有pid字段和document 字段，document是句子字符串的列表，每个字符串逻辑上是一个Sentence。                                                                                                                       |\n",
    "| `train.json`                   | 百度 DuReader V2.0 (筛选处理后)                     | 问答语料，包含问题、对应的答案以及相关的文档 `pid` 。用于训练检索评估、候选答案句排序和答案抽取模块。                                                    | 也是JSONL，每一行的对象是question、pid、          answer_sentence、   answer、qid组成的，其中answer_sentence是句子的列表，但是一般是一个句子，answer和question是单个字符串，pid和qid是int                                                          |\n",
    "| `train_questions.txt`          | 哈工大信息检索研究室问答系统问题集                      | 用于训练问题分类器的问题及对应的类别标签 。类别标签形如 `HUM_PERSON`，下划线前为粗分类标签 。                                                   | 文本文件 (.txt)。每行可能是一个标签和一个问题，由制表符或空格分隔。标签看起来是这个问题的类型，比如DES_DEFINITION是要求别人描述定义的问题。                                                                                                       |\n",
    "| `test_questions.txt`           | 哈工大信息检索研究室问答系统问题集                      | 用于测试问题分类器性能的问题及对应的类别标签 。                                                                    | 与`train_questions.txt`一样。                                                                                                     |\n",
    "\n",
    "\n",
    "主要来自两个数据集 ：\n",
    "\n",
    "- DuReader V2.0：这是百度在2017年发布的一个阅读理解数据集 。本次实验使用的 passages_multi_sentences.json 和 train.json 就是从这个数据集中筛选和处理得到的 。\n",
    "- 哈工大信息检索研究室问答系统问题集：这是一个由哈工大信息检索研究室在2004年公开的数据集 。实验中的问题分类语料（即 train_questions.txt 和 test_questions.txt）来源于此 。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## 代码规范说明\n",
    "\n",
    "在我们实现函数过程中，函数的docstring应当遵循fastai规范而不是numpy规范，这样简洁清晰，不会Repeat yourself。相应的哲学和具体区别可以看 \n",
    "https://nbdev.fast.ai/tutorials/best_practices.html#keep-docstrings-short-elaborate-in-separate-cells\n",
    "\n",
    "\n",
    "为了让代码清晰规范，在作业开始前，使用 `ruff format`格式化助教老师给的代码; \n",
    "\n",
    "![alt text](image-2.png)\n",
    "\n",
    "\n",
    "很好，这次代码格式化没有报错。\n",
    "\n",
    "Pylance 似乎也没有明显问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431382e5",
   "metadata": {},
   "source": [
    "## 实验环境准备\n",
    "\n",
    "采用上次的作业专属环境，为了跑通最新方法，使用3.12 和 torch 2.7\n",
    "\n",
    "```bash\n",
    "conda create -n assignments python=3.12\n",
    "conda activate assignments\n",
    "pip install -r ../requirements.txt\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "pip install -U git+https://github.com/TorchRWKV/flash-linear-attention\n",
    "```\n",
    "\n",
    "本次作业似乎没有新的依赖，只是用到了 torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## 原理回顾和课件复习\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f03fe",
   "metadata": {},
   "source": [
    "在作业9. 中我们进行了简单的复习，这里不再赘述。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154823b4",
   "metadata": {},
   "source": [
    "## 1. 建立文档检索系统\n",
    "\n",
    "首先我们来理解这个子任务的要求，\n",
    "我注意到，train.json中的样本的pid，其实就是passages中的pid，是一一对应的。\n",
    "我们要构建一个函数，输入train.json中的question，中间经过关键词提取, 系统读取passages_multi_sentences.json中的文档库，返回与问题相关的文档的pid列表， 使用Recall指标来评估检索系统的性能。\n",
    "\n",
    "这里没有让我们用深度学习，而是传统的检索。\n",
    "\n",
    "首先需要去除停用词，停用词是指在文本中出现频率非常高，但对文本意义贡献不大的词语（如“的”、“是”、“在”等）。去除它们可以减少噪音，降低计算量。注意到助教已经提供了停用词列表 `stopwords.txt`，我们可以直接使用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2e420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(filepath):\n",
    "    \"\"\"Loads stopwords from a file into a set.\"\"\"\n",
    "    stopwords_set = set()\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: Stopwords file not found at {filepath}. Proceeding without stopwords.\")\n",
    "        return stopwords_set\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stopwords_set.add(line.strip())\n",
    "    print(f\"Loaded {len(stopwords_set)} stopwords.\")\n",
    "    return stopwords_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bbb8e5",
   "metadata": {},
   "source": [
    "在助教的作业要求中，建议我们使用Whoosh https://whoosh.readthedocs.io/en/latest/intro.html\n",
    "\n",
    "Whoosh，像其他搜索引擎库一样，通常在构建索引时内部处理文本分析（分词、去除停用词、小写化、词干提取等）。这是通过其 Schema 中的 FieldType（尤其是 TEXT 类型字段）及其关联的 Analyzer 来完成的。\n",
    "\n",
    "作业要求中助教让我们实现的 Jieb分词和停用词去除逻辑，实际上正是 Whoosh Analyzer 需要做的事情。所以更理想和“Whoosh原生”的做法是，我们不需要显示地写一个 preprocess_data.py 把输出（词列表）直接保存成一个中间文件让 Whoosh 去读取。而是，我们将原始文本（passages_multi_sentences.json 中的句子）直接喂给 Whoosh，然后配置 Whoosh 使用一个包含 Jieba 分词和我们停用词表的自定义 Analyzer。这样，索引的建立和文本分析的逻辑就都封装在 Whoosh 内部了，更加清晰和高效。\n",
    "\n",
    "查阅 https://whoosh.readthedocs.io/en/latest/analysis.html\n",
    "\n",
    "接口需要我们对str进行yield Token对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0978799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step a) Part 1: Preprocessing logic for Whoosh Analyzer and Questions ---\n",
    "class ChineseTokenizer(Tokenizer):\n",
    "    \"\"\"Custom Whoosh Tokenizer for Chinese text using Jieba.\"\"\"\n",
    "    def __call__(self, text:str, **kargs):\n",
    "        words = jieba.lcut(text) \n",
    "        token_instance = Token() # Create one Token object to reuse\n",
    "        current_pos = 0\n",
    "        for word in words:\n",
    "            word_clean = word.strip()\n",
    "            # Ensure it's not a stopword and not just whitespace\n",
    "            if word_clean and word_clean.lower() not in STOPWORDS and not word_clean.isspace():\n",
    "                token_instance.text = word_clean\n",
    "                token_instance.original = word_clean\n",
    "                token_instance.pos = current_pos # Assign current position\n",
    "                # token_instance.startchar = start_char # Optional: if you track character offsets\n",
    "                # token_instance.endchar = end_char   # Optional\n",
    "                yield token_instance\n",
    "                current_pos += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f81c9",
   "metadata": {},
   "source": [
    "Whoosh Schema 我们需要定义一个 Schema 来描述文档的结构。至少会有一个字段用于存储文档ID (pid)，以及一个 TEXT 字段用于存储文档内容，这个内容字段将使用我们自定义的 Analyzer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031818fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema(\n",
    "        pid=ID(stored=True, unique=True),\n",
    "        content=TEXT(analyzer=chinese_analyzer(), stored=False) # Content not stored to save space\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11208de0",
   "metadata": {},
   "source": [
    "现在对于每条记录，提取 pid 和 document (句子列表)。将句子列表合并为单个字符串。\n",
    "使用 IndexWriter.add_document() 将 pid 和合并后的文档内容添加到索引中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185726ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_document_text = \" \".join(document_sentences)\n",
    "\n",
    "                    writer.add_document(pid=str(pid), content=full_document_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9b9a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4419d32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcf2a175",
   "metadata": {},
   "source": [
    "现在我们运行\n",
    "\n",
    "python src/retrieval_system.py\n",
    "\n",
    "![alt text](image-3.png)\n",
    "\n",
    "![alt text](image-4.png)\n",
    "\n",
    "![alt text](image-5.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
