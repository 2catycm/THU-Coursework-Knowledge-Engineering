import os
import json
import logging
import random
import shutil # For directory operations if needed for caching
from typing import List, Dict, Any, Tuple, Optional

import torch
import evaluate
from datasets import Dataset # Only if needed for specific metric formatting
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM # For SFT model
from sentence_transformers.cross_encoder import CrossEncoder # For Ranker
from whoosh.index import open_dir
from whoosh.qparser import QueryParser, MultifieldParser
from whoosh.searching import Searcher
from sklearn.model_selection import train_test_split # For splitting train.json for eval
from tqdm.auto import tqdm
import numpy as np
import jieba
from rouge_chinese import Rouge
from collections import Counter


# --- 0. Configuration & Logging ---
logging.basicConfig(format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
                    datefmt="%Y-%m-%d %H:%M:%S",
                    level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Model & Data Paths ---
# Step 1: Document Retrieval (Whoosh)
WHOOSH_INDEX_DIR = "./indexdir" # Your Whoosh index directory
PASSAGES_FILE = "./data/passages_multi_sentences.json"

# Step 2: Question Classifier (QC)
# Choose the QC model type and path that performed best for you
# QC_MODEL_TYPE_NAME = "Qwen2_for_QC" # Example if you named it this
QC_MODEL_TYPE_NAME_CLEAN = "qwen3_qc_output_debug" 
QUESTION_CLASSIFIER_PATH = f"./{QC_MODEL_TYPE_NAME_CLEAN}/final_model_for_pipeline"
QC_TRUST_REMOTE_CODE = "qwen" in QC_MODEL_TYPE_NAME_CLEAN.lower()

# Step 3: Sentence Ranker (SR - CrossEncoder)
SR_BASE_MODEL_TYPE_FOR_PATH = "GTE" # The base used for the CrossEncoder path
SR_USE_QUESTION_TYPE_IN_RANKER = True # Did your best ranker use q_type?
SR_OUTPUT_DIR_BASE = f"./sentence_ranker_output_{SR_BASE_MODEL_TYPE_FOR_PATH}"
if SR_USE_QUESTION_TYPE_IN_RANKER:
    SR_OUTPUT_DIR_BASE += "_with_qtype"
SENTENCE_RANKER_PATH = os.path.join(SR_OUTPUT_DIR_BASE, "final_model")
# The HF model name that was the base for the CrossEncoder (needed for tokenizer if not saved with CrossEncoder explicitly)
SR_CROSS_ENCODER_HF_MODEL_NAME = "Alibaba-NLP/gte-multilingual-reranker-base"
SR_TRUST_REMOTE_CODE = True


# Step 4: Answer Extractor (AE - SFT Generative Model)
AE_SFT_MODEL_TYPE_FOR_PATH = "Qwen3_0.6B" # The SFT model type used
AE_OUTPUT_DIR_BASE = f"./sft_model_output_{AE_SFT_MODEL_TYPE_FOR_PATH.lower().replace('/', '_')}"
ANSWER_EXTRACTOR_PATH = os.path.join(AE_OUTPUT_DIR_BASE, "final_sft_model")
AE_TOKENIZER_FOR_PROMPT_NAME = "Qwen/Qwen3-0.6B"
AE_TRUST_REMOTE_CODE = "qwen" in AE_SFT_MODEL_TYPE_FOR_PATH.lower()


# Evaluation Data
EVAL_JSON_FILE = "./data/train.json"
NUM_EVAL_SAMPLES = 100 # Number of samples from train.json to evaluate on, set to None for all

# Pipeline Parameters
NUM_RETRIEVED_DOCS_FROM_WHOOSH = 3
TOP_K_SENTENCES_FROM_RANKER = 1 # How many top sentences to feed to QA model
GENERATION_MAX_NEW_TOKENS_AE = 60 # Max tokens for answer generated by AE

# --- Global Model Placeholders ---
passages_data: Dict[str, List[str]] = {}
whoosh_ix: Optional[Any] = None
whoosh_q_parser: Optional[QueryParser] = None

qc_pipeline_global: Optional[pipeline] = None
sentence_ranker_model_global: Optional[CrossEncoder] = None
# Tokenizer for sentence ranker (CrossEncoder loads its own or uses from HF model)

answer_extractor_pipeline_global: Optional[pipeline] = None
# Tokenizer for formatting AE prompts (especially chat templates)
ae_tokenizer_for_chat_template_global: Optional[AutoTokenizer] = None


# --- Helper functions from previous scripts ---

def load_passages(passages_file_path: str) -> Dict[str, List[str]]:
    _passages_dict = {}
    try:
        with open(passages_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line)
                _passages_dict[str(data['pid'])] = data['document']
        logger.info(f"Loaded {len(_passages_dict)} documents from {passages_file_path}")
    except FileNotFoundError:
        logger.error(f"Passages file not found at {passages_file_path}. Cannot proceed.")
        raise
    return _passages_dict

# Qwen3/SFT Answer Parsing (from step4 text_generation_sft.py)
THINK_END_TAG_STR_AE = "</think>"
def parse_qwen_final_answer_for_pipeline(full_generated_text: str, prompt_text_for_generation: str) -> str:
    pure_completion = full_generated_text
    # Check if the model echoed the prompt (common for text-generation pipelines)
    # The pipeline prompt might itself be slightly different from generation_prompt_text if it adds BOS etc.
    # A robust way is to check if the core user input part of generation_prompt_text is at the start
    
    # A simpler heuristic for pipeline outputs which often ONLY return new tokens:
    # For now, let's assume pipeline_prompt_text is what was fed, and full_generated_text starts with it OR is only completion.
    # If the pipeline returns only new tokens, pure_completion is already just that.
    # If it returns prompt + new tokens, we strip prompt.
    if full_generated_text.startswith(prompt_text_for_generation):
        pure_completion = full_generated_text[len(prompt_text_for_generation):]
    
    parts = pure_completion.split(THINK_END_TAG_STR_AE, 1)
    final_answer_part = parts[1] if len(parts) > 1 else pure_completion
    final_answer_lines = [line for line in final_answer_part.splitlines() if line.strip()]
    cleaned_answer = "\n".join(final_answer_lines).strip()
    return cleaned_answer


# Metrics computation (from step4 text_generation_sft.py)
def tokenize_for_eval_metric(text: str) -> List[str]:
    if not text: return []
    return list(jieba.cut(text))

def compute_generative_metrics_for_pipeline(predictions: List[str], references_texts: List[List[str]]):
    em_scores = []
    f1_scores = []
    squad_predictions_fmt = []
    squad_references_fmt = []

    for i, (pred_text, true_ans_list) in enumerate(zip(predictions, references_texts)):
        current_id = f"eval_{i}" # Create a dummy ID for SQuAD metric
        
        # For EM/F1 to be calculated by SQuAD metric (which is token-based)
        # Prediction format for SQuAD metric
        squad_predictions_fmt.append({"id": current_id, "prediction_text": pred_text})
        # Reference format for SQuAD metric (needs 'answers' dict with 'text' list and 'answer_start' list)
        # Since we only have text, we can't compute 'answer_start' easily here for SQuAD tool.
        # So, we'll calculate EM and token-F1 manually as before.
        
        # Manual EM
        current_em = 0
        if pred_text in true_ans_list: # true_ans_list is List[str]
            current_em = 1
        em_scores.append(current_em)

        # Manual Token-based F1 (max over references)
        pred_tokens = tokenize_for_eval_metric(pred_text)
        max_f1_for_item = 0.0
        if not true_ans_list or not any(true_ans_list): # No ground truth answers
             max_f1_for_item = 1.0 if not pred_tokens else 0.0
        else:
            for true_ans_text in true_ans_list:
                if not true_ans_text: # Skip empty reference string
                    if not pred_tokens: max_f1_for_item = max(max_f1_for_item, 1.0)
                    else: max_f1_for_item = max(max_f1_for_item, 0.0)
                    continue

                true_tokens = tokenize_for_eval_metric(true_ans_text)
                if not true_tokens:
                    _f1 = 1.0 if not pred_tokens else 0.0
                elif not pred_tokens:
                    _f1 = 0.0
                else:
                    common = Counter(pred_tokens) & Counter(true_tokens)
                    num_common = sum(common.values())
                    precision = num_common / len(pred_tokens)
                    recall = num_common / len(true_tokens)
                    _f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
                if _f1 > max_f1_for_item:
                    max_f1_for_item = _f1
        f1_scores.append(max_f1_for_item)

    results = {
        "exact_match": np.mean(em_scores) if em_scores else 0.0,
        "f1": np.mean(f1_scores) if f1_scores else 0.0,
    }

    try:
        bleu_metric = evaluate.load("bleu")
        bleu_results = bleu_metric.compute(predictions=predictions, references=references_texts)
        results["bleu"] = bleu_results.get("bleu", 0.0)
    except Exception as e_bleu:
        logger.error(f"BLEU computation error: {e_bleu}")
        results["bleu"] = 0.0
    
    try:
        rouge_predictions_tokenized = [" ".join(tokenize_for_eval_metric(p)) for p in predictions]
        rouge_references_tokenized = [[" ".join(tokenize_for_eval_metric(r)) for r in ref_list] for ref_list in references_texts]
        rouge_calculator = Rouge()
        if rouge_predictions_tokenized and rouge_references_tokenized:
            # Use first reference for rouge_chinese get_scores avg
            rouge_scores_all = rouge_calculator.get_scores(rouge_predictions_tokenized, [r[0] for r in rouge_references_tokenized if r], avg=True)
            results["rouge-l_f"] = rouge_scores_all['rouge-l']['f']
        else:
            results["rouge-l_f"] = 0.0
    except Exception as e_rouge:
        logger.error(f"ROUGE computation error: {e_rouge}")
        results["rouge-l_f"] = 0.0
        
    return results


# --- 1. Document Retrieval Functions ---
def initialize_retriever():
    global whoosh_ix, whoosh_q_parser
    if not os.path.exists(WHOOSH_INDEX_DIR):
        logger.error(f"Whoosh index directory not found: {WHOOSH_INDEX_DIR}. Document retrieval will fail.")
        logger.error("Please run the document indexing script (Step 1) first.")
        whoosh_ix = None
        return False
    try:
        whoosh_ix = open_dir(WHOOSH_INDEX_DIR)
        # Using MultifieldParser to search in 'content' and potentially 'title' if you indexed it.
        # For simplicity, assuming only 'content' was indexed as per typical Step 1.
        whoosh_q_parser = QueryParser("content", schema=whoosh_ix.schema) # type: ignore
        logger.info(f"Whoosh index loaded successfully from {WHOOSH_INDEX_DIR}.")
        return True
    except Exception as e:
        logger.error(f"Error loading Whoosh index: {e}")
        whoosh_ix = None
        return False

from whoosh.analysis import Tokenizer, Token
STOPWORDS_FILE = os.path.join("./data", 'stopwords.txt')

def load_stopwords(filepath):
    """Loads stopwords from a file into a set."""
    stopwords_set = set()
    if not os.path.exists(filepath):
        print(f"Warning: Stopwords file not found at {filepath}. Proceeding without stopwords.")
        return stopwords_set
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            stopwords_set.add(line.strip())
    print(f"Loaded {len(stopwords_set)} stopwords.")
    return stopwords_set

# Global stopwords set
STOPWORDS = load_stopwords(STOPWORDS_FILE)

class ChineseTokenizer(Tokenizer):
    """Custom Whoosh Tokenizer for Chinese text using Jieba."""
    def __call__(self, text:str, **kargs):
        words = jieba.lcut(text) 
        token_instance = Token() # Create one Token object to reuse
        current_pos = 0
        for word in words:
            word_clean = word.strip()
            # Ensure it's not a stopword and not just whitespace
            if word_clean and word_clean.lower() not in STOPWORDS and not word_clean.isspace():
                token_instance.text = word_clean
                token_instance.original = word_clean
                token_instance.pos = current_pos # Assign current position
                # token_instance.startchar = start_char # Optional: if you track character offsets
                # token_instance.endchar = end_char   # Optional
                yield token_instance
                current_pos += 1


def retrieve_document_pids(question_text: str, top_n: int = NUM_RETRIEVED_DOCS_FROM_WHOOSH) -> List[str]:
    if whoosh_ix is None or whoosh_q_parser is None:
        logger.warning("Whoosh retriever not initialized. Returning empty list.")
        return []
    try:
        query = whoosh_q_parser.parse(question_text) # Simple keyword query
        with whoosh_ix.searcher() as searcher:
            results = searcher.search(query, limit=top_n)
            retrieved_pids = [hit['pid'] for hit in results]
            logger.debug(f"Retrieved PIDs for '{question_text[:30]}...': {retrieved_pids}")
            return retrieved_pids
    except Exception as e:
        logger.error(f"Error during Whoosh document retrieval: {e}")
        return []

# --- 2. Question Classification Function ---
def initialize_question_classifier():
    global qc_pipeline_global
    if not os.path.exists(QUESTION_CLASSIFIER_PATH):
        logger.error(f"Question Classifier model not found at {QUESTION_CLASSIFIER_PATH}. QC step will be skipped.")
        return False
    try:
        qc_device = 0 if torch.cuda.is_available() else -1
        qc_pipeline_global = pipeline(
            "text-classification",
            model=QUESTION_CLASSIFIER_PATH,
            tokenizer=QUESTION_CLASSIFIER_PATH,
            device=qc_device,
            trust_remote_code=QC_TRUST_REMOTE_CODE
        )
        logger.info(f"Question Classifier pipeline loaded from {QUESTION_CLASSIFIER_PATH}.")
        return True
    except Exception as e:
        logger.error(f"Error loading Question Classifier pipeline: {e}")
        return False

def classify_question(question_text: str) -> str:
    if qc_pipeline_global is None:
        logger.warning("Question Classifier not initialized. Returning UNKNOWN_TYPE.")
        return "UNKNOWN_TYPE"
    try:
        prediction = qc_pipeline_global(question_text)
        # Assuming pipeline returns [{'label': 'HUM_PERSON', 'score': ...}]
        if prediction and isinstance(prediction, list) and prediction[0].get('label'):
            # We need the coarse type for the ranker's prompt format
            coarse_type = prediction[0]['label'].split('_')[0]
            logger.debug(f"Classified question '{question_text[:30]}...' as: {coarse_type} (Fine: {prediction[0]['label']})")
            return coarse_type
    except Exception as e:
        logger.error(f"Error during question classification: {e}")
    return "UNKNOWN_TYPE"


# --- 3. Sentence Ranking Functions ---
def initialize_sentence_ranker():
    global sentence_ranker_model_global
    if not os.path.exists(SENTENCE_RANKER_PATH):
        logger.error(f"Sentence Ranker model not found at {SENTENCE_RANKER_PATH}. Ranking will be skipped.")
        return False
    try:
        # CrossEncoder loads the underlying HF model, num_labels should match how it was saved.
        # If saved with save_pretrained, it saves config.json with num_labels.
        sentence_ranker_model_global = CrossEncoder(
            SENTENCE_RANKER_PATH, # Path to the saved CrossEncoder model directory
            # num_labels=1, # This should be loaded from config if saved properly
            # max_length set during training, should also be loaded from config
            device=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
            trust_remote_code=SR_TRUST_REMOTE_CODE # If base model needed it
        )
        # If the CrossEncoder was saved using `model.save_pretrained(path)`,
        # it saves its config, tokenizer, and the underlying HF model.
        # The CrossEncoder class can then be initialized with just the path.
        logger.info(f"Sentence Ranker (CrossEncoder) loaded from {SENTENCE_RANKER_PATH}.")
        return True
    except Exception as e:
        logger.error(f"Error loading Sentence Ranker: {e}")
        import traceback
        traceback.print_exc()
        return False


def rank_candidate_sentences(question_text: str, question_type: str, candidate_sentences: List[str]) -> List[str]:
    if sentence_ranker_model_global is None or not candidate_sentences:
        logger.warning("Sentence Ranker not initialized or no candidate sentences. Returning unranked or empty list.")
        return candidate_sentences # Return as is, or first few if too many

    # Prepare input for ranker
    query_for_ranker = question_text
    if SR_USE_QUESTION_TYPE_IN_RANKER and question_type and question_type != "UNKNOWN_TYPE":
        query_for_ranker = f"[TYPE:{question_type}] {question_text}"
    
    sentence_pairs = [[query_for_ranker, sent] for sent in candidate_sentences]
    
    try:
        logger.debug(f"Ranking {len(sentence_pairs)} pairs for question '{question_text[:30]}...'")
        # CrossEncoder.predict returns scores (higher is better)
        scores = sentence_ranker_model_global.predict(sentence_pairs, show_progress_bar=False)
        
        scored_sentences = list(zip(scores, candidate_sentences))
        # Sort by score in descending order
        scored_sentences.sort(key=lambda x: x[0], reverse=True)
        
        ranked_sentences = [sent for score, sent in scored_sentences]
        logger.debug(f"Top ranked sentence for '{question_text[:30]}...': '{ranked_sentences[0][:50]}...' with score {scored_sentences[0][0]:.4f}")
        return ranked_sentences
    except Exception as e:
        logger.error(f"Error during sentence ranking: {e}")
        return candidate_sentences # Fallback to unranked

# --- 4. Answer Extraction Functions ---
def initialize_answer_extractor():
    global answer_extractor_pipeline_global, ae_tokenizer_for_chat_template_global
    if not os.path.exists(ANSWER_EXTRACTOR_PATH):
        logger.error(f"Answer Extractor model not found at {ANSWER_EXTRACTOR_PATH}. Answer extraction will fail.")
        return False
    try:
        ae_device = 0 if torch.cuda.is_available() else -1
        # Load tokenizer separately for consistent prompt formatting if needed for parsing
        ae_tokenizer_for_chat_template_global = AutoTokenizer.from_pretrained(
            AE_TOKENIZER_FOR_PROMPT_NAME, # Use the tokenizer name it was trained with for chat template
            trust_remote_code=AE_TRUST_REMOTE_CODE,
            padding_side='left'
        )
        if ae_tokenizer_for_chat_template_global.pad_token is None:
            ae_tokenizer_for_chat_template_global.pad_token = ae_tokenizer_for_chat_template_global.eos_token \
                if ae_tokenizer_for_chat_template_global.eos_token is not None else "[PAD]"

        answer_extractor_pipeline_global = pipeline(
            "text-generation", # SFT model is a text generator
            model=ANSWER_EXTRACTOR_PATH,
            tokenizer=ae_tokenizer_for_chat_template_global, # Use the loaded tokenizer
            device=ae_device,
            trust_remote_code=AE_TRUST_REMOTE_CODE
        )
        logger.info(f"Answer Extractor (SFT text-generation pipeline) loaded from {ANSWER_EXTRACTOR_PATH}.")
        return True
    except Exception as e:
        logger.error(f"Error loading Answer Extractor pipeline: {e}")
        import traceback
        traceback.print_exc()

        return False

def extract_answer_from_sentence(question_text: str, context_sentence: str) -> str:
    if answer_extractor_pipeline_global is None or ae_tokenizer_for_chat_template_global is None:
        logger.warning("Answer Extractor not initialized. Returning empty answer.")
        return "无法提取答案 (模型未加载)。"

    # Format prompt for SFT model (allowing thinking, then parsing)
    system_message = "你是一个乐于助人的问答助手。请根据提供的上下文和问题，简洁地回答问题。答案必须从上下文中获取或基于上下文推断。"
    user_message_content = f"上下文：\n{context_sentence}\n\n问题：\n{question_text}\n\n请根据以上信息回答。"
    messages_for_generation = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message_content}
    ]
    
    try:
        # Use enable_thinking=True as per last SFT script design for eval
        generation_prompt = ae_tokenizer_for_chat_template_global.apply_chat_template(
            messages_for_generation, tokenize=False, add_generation_prompt=True, enable_thinking=True
        )
    except Exception as e_tmpl:
        logger.error(f"Error applying chat template for AE: {e_tmpl}. Using generic prompt.")
        generation_prompt = f"<s>[INST] 系统: {system_message}\n用户: {user_message_content} [/INST]\n" # Fallback
    
    try:
        logger.debug(f"AE Pipeline input prompt: {generation_prompt[:200]}...")
        # Pipeline for text-generation usually takes the prompt string.
        # Ensure generation parameters are sensible.
        pipeline_output = answer_extractor_pipeline_global(
            generation_prompt,
            max_new_tokens=GENERATION_MAX_NEW_TOKENS_AE,
            pad_token_id=ae_tokenizer_for_chat_template_global.pad_token_id,
            eos_token_id=ae_tokenizer_for_chat_template_global.eos_token_id,
            num_return_sequences=1,
            do_sample=False # For more deterministic output during eval
        )
        full_generated_text = pipeline_output[0]['generated_text']
        logger.debug(f"AE Pipeline full output: {full_generated_text[:300]}...")
        
        # Parse the final answer (stripping prompt and <think> block)
        final_answer = parse_qwen_final_answer_for_pipeline(full_generated_text, generation_prompt)
        logger.debug(f"Extracted answer for '{question_text[:30]}...': '{final_answer[:50]}...'")
        return final_answer if final_answer else "未能从句子中提取到明确答案。"
    except Exception as e:
        logger.error(f"Error during answer extraction: {e}")
        import traceback
        traceback.print_exc()
        return "答案提取时发生错误。"

# --- 5. Integrated QA Pipeline ---
def run_full_qa_pipeline(question_text: str) -> str:
    logger.info(f"\n--- Running Full QA Pipeline for Question: '{question_text}' ---")

    # Step 1: Document Retrieval
    retrieved_pids = retrieve_document_pids(question_text)
    if not retrieved_pids:
        logger.warning("Pipeline: No documents retrieved.")
        return "抱歉，未能找到相关文档来回答您的问题。"
    logger.info(f"Pipeline: Retrieved PIDs: {retrieved_pids}")

    # Step 2: Question Classification
    question_type = classify_question(question_text)
    logger.info(f"Pipeline: Classified question type: {question_type}")

    # Step 3: Candidate Sentence Ranking
    all_candidate_sentences_from_docs: List[str] = []
    for pid in retrieved_pids:
        doc_sents = passages_data.get(pid)
        if doc_sents:
            all_candidate_sentences_from_docs.extend(doc_sents)
    
    if not all_candidate_sentences_from_docs:
        logger.warning("Pipeline: No candidate sentences found in retrieved documents.")
        return "抱歉，虽然找到了相关文档，但未能从中定位到候选句子。"

    ranked_sentences = rank_candidate_sentences(question_text, question_type, all_candidate_sentences_from_docs)
    if not ranked_sentences:
        logger.warning("Pipeline: Sentence ranking returned no sentences.")
        return "抱歉，未能从候选句子中排序得到最相关的句子。"
    
    top_sentence_for_qa = ranked_sentences[0] # Take the top 1 sentence
    logger.info(f"Pipeline: Top ranked sentence: '{top_sentence_for_qa[:100]}...'")
    
    # For TOP_K_SENTENCES_FOR_QA > 1, you might concatenate them or process separately
    # Here, we use only the top 1.
    context_for_ae = top_sentence_for_qa

    # Step 4: Answer Extraction
    final_answer = extract_answer_from_sentence(question_text, context_for_ae)
    logger.info(f"Pipeline: Final extracted answer: '{final_answer}'")
    
    return final_answer


# --- 6. Evaluation Loop ---
def evaluate_pipeline_on_train_json(eval_items: List[Dict]):
    logger.info(f"\n--- Starting Pipeline Evaluation on {len(eval_items)} samples ---")
    
    pipeline_predictions = []
    ground_truth_references = [] # List of lists of strings for BLEU/ROUGE

    for item in tqdm(eval_items, desc="Evaluating Full Pipeline"):
        question = item.get("question")
        true_answers_list = item.get("answer", []) # DuReader answer can be list
        true_answer_text = true_answers_list[0] if isinstance(true_answers_list, list) and true_answers_list else \
                           true_answers_list if isinstance(true_answers_list, str) else ""

        if not question or not true_answer_text:
            continue

        predicted_answer = run_full_qa_pipeline(question)
        pipeline_predictions.append(predicted_answer)
        ground_truth_references.append([true_answer_text]) # BLEU/ROUGE expect list of refs

    # Compute metrics
    if pipeline_predictions:
        metrics = compute_generative_metrics_for_pipeline(pipeline_predictions, ground_truth_references)
        logger.info("--- Overall Pipeline Evaluation Metrics ---")
        for metric_name, value in metrics.items():
            logger.info(f"  {metric_name}: {value:.4f}")
    else:
        logger.warning("No predictions made during pipeline evaluation. Metrics cannot be computed.")

# --- 7. Main Execution ---
def load_all_components():
    global passages_data
    logger.info("--- Initializing All QA Pipeline Components ---")
    passages_data = load_passages(PASSAGES_FILE)
    
    retriever_ok = initialize_retriever()
    qc_ok = initialize_question_classifier()
    ranker_ok = initialize_sentence_ranker()
    ae_ok = initialize_answer_extractor()
    
    if not (retriever_ok and qc_ok and ranker_ok and ae_ok):
        logger.error("One or more pipeline components failed to initialize. Pipeline may not function correctly.")
        # exit(1) # Or handle more gracefully
        return False
    logger.info("--- All QA Pipeline Components Initialized Successfully ---")
    return True


if __name__ == "__main__":
    if not load_all_components():
        logger.fatal("Exiting due to component initialization failure.")
        exit() # Exit if components can't load

    # Test with a few sample questions
    sample_questions_to_test = [
        "左权将军殉难处的地址在哪里？",
        "清朝的开国皇帝是谁？",
        "什么是人工智能？"
    ]
    for q_text in sample_questions_to_test:
        answer = run_full_qa_pipeline(q_text)
        print(f"\nQ: {q_text}\nA: {answer}\n" + "="*30)

    # Evaluate on a subset of train.json
    logger.info(f"\n--- Preparing for Full Pipeline Evaluation on {EVAL_JSON_FILE} ---")
    with open(EVAL_JSON_FILE, 'r', encoding='utf-8') as f:
        all_eval_items = [json.loads(line) for line in f]
    
    if NUM_EVAL_SAMPLES is not None and NUM_EVAL_SAMPLES < len(all_eval_items):
        logger.info(f"Sampling {NUM_EVAL_SAMPLES} items for evaluation.")
        # Stratified sampling might be better if data is imbalanced, but random for now
        items_for_pipeline_eval = random.sample(all_eval_items, NUM_EVAL_SAMPLES)
    else:
        items_for_pipeline_eval = all_eval_items
    
    if items_for_pipeline_eval:
        evaluate_pipeline_on_train_json(items_for_pipeline_eval)
    else:
        logger.warning("No items selected for pipeline evaluation.")
        
    logger.info("--- Full QA Pipeline Script Finished ---")