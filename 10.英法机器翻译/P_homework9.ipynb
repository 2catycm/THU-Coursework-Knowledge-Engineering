{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 知识工程-作业10 英法中文翻译\n",
    "2024214500 叶璨铭\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 代码与文档格式说明\n",
    "\n",
    "> 本文档使用Jupyter Notebook编写，遵循Diátaxis 系统 Notebook实践 https://nbdev.fast.ai/tutorials/best_practices.html，所以同时包括了实验文档和实验代码。\n",
    "\n",
    "> 本文档理论上支持多个格式，包括ipynb, docx, pdf 等。您在阅读本文档时，可以选择您喜欢的格式来进行阅读，建议您使用 Visual Studio Code (或者其他支持jupyter notebook的IDE, 但是VSCode阅读体验最佳) 打开 `ipynb`格式的文档来进行阅读。\n",
    "\n",
    "> 为了记录我们自己修改了哪些地方，使用git进行版本控制，这样可以清晰地看出我们基于助教的代码在哪些位置进行了修改，有些修改是实现了要求的作业功能，而有些代码是对原本代码进行了重构和优化。我将我在知识工程课程的代码，在作业截止DDL之后，开源到 https://github.com/2catycm/THU-Coursework-Knowledge-Engineering.git ，方便各位同学一起学习讨论。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## 代码规范说明\n",
    "\n",
    "在我们实现函数过程中，函数的docstring应当遵循fastai规范而不是numpy规范，这样简洁清晰，不会Repeat yourself。相应的哲学和具体区别可以看 \n",
    "https://nbdev.fast.ai/tutorials/best_practices.html#keep-docstrings-short-elaborate-in-separate-cells\n",
    "\n",
    "\n",
    "为了让代码清晰规范，在作业开始前，使用 `ruff format`格式化助教老师给的代码; \n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "\n",
    "很好，这次代码格式化没有报错。\n",
    "\n",
    "Pylance 似乎也没有明显问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431382e5",
   "metadata": {},
   "source": [
    "## 实验环境准备\n",
    "\n",
    "采用上次的作业专属环境，为了跑通最新方法，使用3.12 和 torch 2.6\n",
    "\n",
    "```bash\n",
    "conda create -n assignments python=3.12\n",
    "conda activate assignments\n",
    "pip install -r ../requirements.txt\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "pip install -U git+https://github.com/TorchRWKV/flash-linear-attention\n",
    "```\n",
    "\n",
    "本次作业似乎没有新的依赖，只是用到了 transformers\n",
    "\n",
    "```python\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## 原理回顾和课件复习\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "课上详细介绍了 Knowledge-based Question Answering\n",
    "\n",
    "首先区分了一下属性和关系，属性是 实体, 属性类型, 字符串； 关系是 实体，关系类型，实体。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 1.1 基于知识图谱的问答系统\n",
    "\n",
    "根据助教老师的要求，我们有四步要做\n",
    "\n",
    "1. 使用 Python 解析 zhishime.json 文件，创建知识图谱\n",
    "2. 实现头实体检索模块（使用正向最大匹配或命名实体识别）\n",
    "3. 使用预训练模型计算问题与关系的相似度\n",
    "4. 提取答案并评估准确性\n",
    "\n",
    "\n",
    "### 1. 使用 python 解析 zhishime.json 文件，并将解析出的 dict 保存为文件\n",
    "\n",
    "注意到 zhishime 实际上是一个 jsonl文件，一行是一个json，每一行是\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"_id\": {\"$oid\": \"5a4a0579b63209a91d0c41c7\"},\n",
    "    \"head\": \"1987大悬案\",\n",
    "    \"relation\": \"监制\",\n",
    "    \"tail\": \"狄诺迪洛伦提斯\\u003cbr/\\u003eRichardRoth\\u003cbr/\\u003e柏尼·威廉斯\",\n",
    "}\n",
    "```\n",
    "\n",
    "这样的格式, 也就是说有 _id, head, relation 和 tail \n",
    "\n",
    "这是知识图谱的典型的三元组。\n",
    "\n",
    "老师已经给了我们 preprocess.py 的初步实现。\n",
    "\n",
    "```python\n",
    "kg = {\n",
    "        \"head2id\": head2id,\n",
    "        \"tail2id\": tail2id,\n",
    "        \"relation2id\": relation2id,\n",
    "        \"relation_triplets\": relation_triplets,\n",
    "    }\n",
    "```\n",
    "\n",
    "这个函数的目标是为了得到 head tail 的id，其实和没处理差不多。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac29f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\"预处理知识图谱数据，构建实体映射和三元组索引\"\"\"\n",
    "    # 初始化数据结构\n",
    "    kg = {\n",
    "        \"head2id\": {},     # 头实体到ID的映射\n",
    "        \"tail2id\": {},     # 尾实体到ID的映射  \n",
    "        \"relation2id\": {}, # 关系到ID的映射\n",
    "        \"relation_triplets\": []  # 存储(hid, rid, tid)形式的三元组\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(\"./processed\", exist_ok=True)\n",
    "        \n",
    "        # 读取原始JSONL文件（注意：使用..表示上级目录）\n",
    "        with open(\"../zhishime.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            # 逐行解析JSON对象（适用于JSON Lines格式）\n",
    "            raw_relation_data = [json.loads(line) for line in f]\n",
    "            \n",
    "            # 等价写法（更易理解）：\n",
    "            # raw_relation_data = []\n",
    "            # for line in f:\n",
    "            #     data = json.loads(line)\n",
    "            #     raw_relation_data.append(data)\n",
    "\n",
    "        # 遍历每个三元组进行索引构建\n",
    "        bar = tqdm(raw_relation_data)\n",
    "        for item in bar:\n",
    "            head = item[\"head\"]\n",
    "            relation = item[\"relation\"]\n",
    "            \n",
    "            # 处理包含换行符的尾实体（示例数据中的<br/>分隔符）\n",
    "            tail = item[\"tail\"].replace(\"\\u003cbr/\\u003e\", \", \")  # 将HTML换行符转换为逗号分隔\n",
    "            \n",
    "            # 构建实体ID映射（自动递增分配ID）\n",
    "            # head2id.setdefault等效写法，但更推荐当前写法\n",
    "            if head not in kg[\"head2id\"]:\n",
    "                kg[\"head2id\"][head] = len(kg[\"head2id\"])\n",
    "                \n",
    "            if tail not in kg[\"tail2id\"]:\n",
    "                kg[\"tail2id\"][tail] = len(kg[\"tail2id\"])\n",
    "                \n",
    "            if relation not in kg[\"relation2id\"]:\n",
    "                kg[\"relation2id\"][relation] = len(kg[\"relation2id\"])\n",
    "            \n",
    "            # 构建三元组索引\n",
    "            hid = kg[\"head2id\"][head]\n",
    "            rid = kg[\"relation2id\"][relation]\n",
    "            tid = kg[\"tail2id\"][tail]\n",
    "            kg[\"relation_triplets\"].append((hid, rid, tid))\n",
    "\n",
    "        # 打印统计信息\n",
    "        print(f\"[统计] 头实体数: {len(kg['head2id'])} | 尾实体数: {len(kg['tail2id'])} | 关系类型数: {len(kg['relation2id'])}\")\n",
    "        print(f\"[统计] 总三元组数: {len(kg['relation_triplets'])}\")\n",
    "\n",
    "        # 保存处理结果\n",
    "        with open(\"./processed/kg.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(kg, json_file, \n",
    "                     ensure_ascii=False,  # 保留非ASCII字符原文\n",
    "                     indent=4)           # 美化格式便于查看\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"错误：未找到原始数据文件，请检查路径是否正确\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON解析错误：第{e.lineno}行数据格式异常，错误详情：{e.msg}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb76c6c",
   "metadata": {},
   "source": [
    "我们稍微重构了下，让整个功能更加稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0969250",
   "metadata": {},
   "source": [
    "![alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc00fb",
   "metadata": {},
   "source": [
    "### 2. 实现头实体检索模块（使用正向最大匹配或命名实体识别）\n",
    "\n",
    "在实现这个函数之前，我们需要明确一个问题，\n",
    "- what：到底什么是头实体？\n",
    "- 在一个question里面，头实体应该是一个还是多个？\n",
    "\n",
    "事实上，这个和对问题的模板预设有关，这次实验我们假设问题类似于 “{HEAD} 的 {RELATION}?” ， 回答是 “{TAIL}”， 所以认为问题中就只有一个实体。例如：\"周杰伦的出生日期是什么？\" → 头实体是\"周杰伦\"。\n",
    "\n",
    "实际上问题中可能包含多个实体，但通常只有一个是头实体\n",
    "\n",
    "例如：\"周杰伦和林俊杰谁的粉丝多？\" → 这种情况复杂一些，可能需要查询多个头实体。\n",
    "\n",
    "\n",
    "刚才我们定义了 \"head2id\": {},     # 头实体到ID的映射\n",
    "\n",
    "所以，我们只需要做一个“多字符串(模式)匹配\", 找到 question 字符串中的那一个实体就好。\n",
    "\n",
    "所谓 正向最大匹配法，就是在问题字符串中，尝试匹配知识图谱中最长的实体名称，优先匹配最长的实体名称，因为这样更可能是完整的实体。 那么相应的逻辑就很简单了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d875695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_head_entity(kg: dict, question: str) -> str:\n",
    "    \"\"\"基于正向最大匹配的头实体识别\n",
    "    Args:\n",
    "        kg: 知识图谱字典，包含head2id等字段\n",
    "        question: 待查询的问题文本\n",
    "    Returns:\n",
    "        匹配成功的头实体字符串，未找到返回None\n",
    "    \"\"\"\n",
    "    # 获取所有可能的头实体\n",
    "    all_heads = list(kg['head2id'].keys())\n",
    "    \n",
    "    # 对头实体按长度排序，优先匹配长的实体\n",
    "    all_heads.sort(key=len, reverse=True)\n",
    "    \n",
    "    # 遍历所有头实体，检查是否在问题中出现\n",
    "    for head in all_heads:\n",
    "        if head in question:\n",
    "            return head\n",
    "    \n",
    "    # 如果没有找到匹配的头实体\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13367106",
   "metadata": {},
   "source": [
    "为了提高处理速度，我们使用缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c37f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_heads = None\n",
    "\n",
    "def search_head_entity(kg: dict, question: str) -> str:\n",
    "    \"\"\"基于正向最大匹配的头实体识别\n",
    "    Args:\n",
    "        kg: 知识图谱字典，包含head2id等字段\n",
    "        question: 待查询的问题文本\n",
    "    Returns:\n",
    "        匹配成功的头实体字符串，未找到返回None\n",
    "    \"\"\"\n",
    "    # 获取所有可能的头实体\n",
    "    global sorted_heads\n",
    "    if sorted_heads is None:\n",
    "        all_heads = list(kg['head2id'].keys())\n",
    "        \n",
    "        # 对头实体按长度排序，优先匹配长的实体\n",
    "        sorted_heads = sorted(all_heads, key=len, reverse=True)\n",
    "    \n",
    "    # 遍历所有头实体，检查是否在问题中出现\n",
    "    for head in sorted_heads:\n",
    "        if head in question:\n",
    "            return head\n",
    "    \n",
    "    # 如果没有找到匹配的头实体\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9ba97",
   "metadata": {},
   "source": [
    "注意到后面的代码是这样的\n",
    "\n",
    "head = search_head_entity(kg, question)\n",
    "        if head is None:\n",
    "\n",
    "为了不让类型有问题（不想用optional），我觉得应该改成\n",
    "if head==\"\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4da9502",
   "metadata": {},
   "source": [
    "### 3. 使用预训练模型计算问题与关系的相似度\n",
    "\n",
    "检查 main.py 代码，发现原本代码这里写错了\n",
    "\n",
    "relations = list(kg[head].keys())\n",
    "\n",
    "我们在kg中根本没有存储从 head 映射到 关系本身的信息！\n",
    "\n",
    "所以我们需要重新修改 preprocess.py 把这个信息假如进去才对。\n",
    "\n",
    "刚才处理json的时候搞了半天id其实是无用功，根本不需要id，核心问题是 一个head有多少个相关的relation。\n",
    "\n",
    "\n",
    "\n",
    "同理，这段代码也不对\n",
    "\n",
    "answer = kg[head][max_relation]\n",
    "\n",
    "这里要解决的核心问题是，给定一个head和一个relation，找到对应的tail的集合。\n",
    "\n",
    "我们首先加上代码\n",
    "\n",
    "```python\n",
    "# 构建头实体到关系的映射\n",
    "if head not in kg[\"head2relations\"]:\n",
    "    kg[\"head2relations\"][head] = set()\n",
    "kg[\"head2relations\"][head].add(relation)\n",
    "\n",
    "# 构建头实体到关系和答案的映射\n",
    "if (head, relation) not in kg[\"head_relations2answers\"]:\n",
    "    kg[\"head_relations2answers\"][(head, relation)] = \"\"\n",
    "kg[\"head_relations2answers\"][(head, relation)] += f\"{tail}, \"\n",
    "```\n",
    "\n",
    "重新运行 python preprocess.py\n",
    "\n",
    "![alt text](image-3.png)\n",
    "\n",
    "那么在json中只能用 list\n",
    "\n",
    "```python\n",
    "if head not in kg[\"head2relations\"]:\n",
    "    kg[\"head2relations\"][head] = list()\n",
    "kg[\"head2relations\"][head].append(relation)\n",
    "```\n",
    "\n",
    "![alt text](image-5.png)\n",
    "\n",
    "json也没有 tuple 的概念\n",
    "\n",
    "![alt text](image-4.png)\n",
    "\n",
    "\n",
    "于是需要修改 \n",
    "\n",
    "```python\n",
    "head_relation = f\"({head}, {relation})\"\n",
    "if head_relation not in kg[\"head_relations2answers\"]:\n",
    "    kg[\"head_relations2answers\"][head_relation] = \"\"\n",
    "kg[\"head_relations2answers\"][head_relation] += f\"{tail}, \"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "现在可以看到\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c47068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['head2id', 'tail2id', 'relation2id', 'relation_triplets'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/processed/kg.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    kg = json.load(f)\n",
    "kg.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f241050",
   "metadata": {},
   "source": [
    "在main中使用 \n",
    "```python\n",
    "        relations = list(kg[\"head2relations\"][head])\n",
    "        ...\n",
    "        answer = kg[\"head_relations2answers\"][(head, max_relation)]\n",
    "```\n",
    "\n",
    "注意json没有元组，所以我们要这样写\n",
    "\n",
    "        answer = kg[\"head_relations2answers\"][f\"({head}, {max_relation})\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339e86e",
   "metadata": {},
   "source": [
    "### 4. 提取答案并评估准确性\n",
    "\n",
    "现在我们终于可以正确运行 main.py 了\n",
    "\n",
    "\n",
    "![alt text](image-6.png)\n",
    "\n",
    "\n",
    "第一个问题就失败了，可能是因为 bert 以为 出生地 是 籍贯。\n",
    "\n",
    "\n",
    "![alt text](image-7.png)\n",
    "\n",
    "bert再次判断错误，以为 机场 是  行政区类别\n",
    "\n",
    "![alt text](image-8.png)\n",
    "\n",
    "王菲的星座是正确的，但是导演关系被误以为是制片地区，而且判断答案是否正确的时候也以为是0.96很高的评分。\n",
    "\n",
    "\n",
    "![alt text](image-9.png)\n",
    "\n",
    "似乎对于英文的相似度，bert比较在行，其他的总会关系理解错误。\n",
    "\n",
    "![alt text](image-10.png)\n",
    "\n",
    "\n",
    "![alt text](image-11.png)\n",
    "\n",
    "最后这个问题我刚才使用了拼接多个关系的方法，所以有逗号，比原答案要多更多东西，但是知识库里面信息有点问题，有个奇怪的“音乐=影像≠音乐”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb4f3e",
   "metadata": {},
   "source": [
    "## 学术问题\n",
    "\n",
    "### 问题1\n",
    "\n",
    "**问题一**：本次作业中所有测试问题都在知识库中有准确答案，然而在实际场景下，这是几乎不可能的。请你回答：有哪些方法可以解决无法被现有知识库很好覆盖的问题？\n",
    "\n",
    "我想，对于人类而言，如果没有学过一个知识，那确实也是不好回答的，只能做两件事情\n",
    "1. 基于现有知识进行推理，给出我对于未知事物性质的猜测，通过想象（幻觉）去给出一个看起来合理的答案。\n",
    "2. 询问外界，比如问老师助教、查阅互联网信息\n",
    "\n",
    "那么对于知识问答系统来说总体思路也是一样，对于知识库中没有的信息，调用联网搜索工具，先阅读网页上的有关内容，然后再回答，如果没有搜到相关的内容, 再自己随便推测一下。\n",
    "\n",
    "具体来说，有这样一些技术\n",
    "- 扩展知识库\n",
    "    - 可以通过信息抽取（Information Extraction, IE）自动从互联网文本中挖掘新实体、新关系，动态扩充知识库。\n",
    "    - 可以利用半监督学习、远程监督（Distant Supervision）等技术从未标注数据中提取知识。\n",
    "- 基于已有知识进行推理，推导出未显式存储的答案。例如利用知识图谱补全（KG Completion）、链式推理（Chain-of-thought Reasoning）等方法。\n",
    "- 结合知识图谱推理与生成模型，先尽可能用结构化数据回答，无法回答时交给生成式模块。利用生成式大模型（如T5, GPT）直接根据用户问题生成合理答案，即使知识图谱中没有明确对应知识。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac245ab3",
   "metadata": {},
   "source": [
    "### 问题2\n",
    "\n",
    "**问题二**：使用预训练 BERT 进行问题检索有哪些优缺点？试从 KBQA 发展历程的角度进行回答。\n",
    "\n",
    "\n",
    "\n",
    "**优点**：\n",
    "\n",
    "- **语义理解更强**：BERT 是基于Transformer结构的深度预训练模型，能够捕捉复杂的上下文信息，比传统的词袋（Bag-of-Words）或浅层模型理解能力更强。\n",
    "- **弱化表面匹配依赖**：传统 KBQA（Knowledge-Based QA）方法往往依赖表面词匹配，BERT 能通过向量空间建模实现更深层次的语义匹配。\n",
    "- **适应性好**：在不同领域，只需少量微调或无监督检索（如Sentence-BERT等）就能迁移到新的任务上。\n",
    "\n",
    "**缺点**：\n",
    "\n",
    "- **检索效率较低**：原生BERT无法高效批量检索，需要引入额外的索引结构（如FAISS）或使用轻量版本（如DistilBERT）。\n",
    "- **难以处理实体特定信息**：BERT擅长理解自然语言，但在结构化实体匹配、别名消歧等方面弱于专门针对实体设计的方法。\n",
    "- **KBQA历史发展视角**：\n",
    "  - **早期 KBQA**：基于规则模板（rule-based）或图遍历（graph traversal），不涉及复杂的语义建模。\n",
    "  - **后期 KBQA**（2018年以后）：随着BERT类模型兴起，开始引入深度语义匹配，但同时也暴露出计算量大、对索引和检索优化需求高的问题。\n",
    "  - 目前越来越多的方法倾向于 **dense retrieval + graph reasoning** 的组合，而不是单纯依赖BERT匹配。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd35b33",
   "metadata": {},
   "source": [
    "### 问题3\n",
    "\n",
    "**问题三**：我们都知道，大语言模型容易出现幻觉，你知道有哪些方法可以缓解大语言模型的幻觉现象？\n",
    "\n",
    "\n",
    "大语言模型（LLMs）容易出现幻觉（Hallucination），即生成逻辑上正确但事实错误的内容。\n",
    "\n",
    "大体来说，最有效的方法通常是**检索+生成**结合，同时在训练、推理两个阶段进行多层次控制。\n",
    "\n",
    "\n",
    "\n",
    "具体缓解方法主要有：\n",
    "\n",
    "1. **检索增强生成（Retrieval-Augmented Generation, RAG）**：\n",
    "   - 在回答问题前检索真实世界文档，将检索到的内容作为条件输入给生成模型，提高回答的事实准确性。\n",
    "\n",
    "2. **事实核查（Fact-Checking）机制**：\n",
    "   - 在模型生成回答后，增加后验验证模块，检查生成内容是否符合事实数据库或知识图谱中的已知知识。\n",
    "\n",
    "3. **训练时使用高质量数据**：\n",
    "   - 使用严格筛选的、包含事实标注的数据集进行微调，减少模型在训练阶段学到错误信息。\n",
    "\n",
    "4. **使用知识注入（Knowledge Injection）**：\n",
    "   - 在训练过程中引入结构化知识（如知识图谱实体嵌入、关系嵌入），增强模型的知识记忆和推理能力。\n",
    "\n",
    "5. **模型约束与提示优化（Prompt Engineering）**：\n",
    "   - 通过在提示词中明确要求“基于事实”“引用来源”等，提高模型生成符合事实的可能性。\n",
    "\n",
    "6. **模型架构优化**：\n",
    "   - 设计专门针对可靠性优化的架构，比如以检索模块为主、生成模块为辅的混合模型。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5ac11",
   "metadata": {},
   "source": [
    "## 2 进阶要求\n",
    "\n",
    "刚才我们看到 bert 模型对于判断相似度还是太弱了，一开始我以为是它不懂中文，但是用的已经是“model = BertModel.from_pretrained(\"bert-base-chinese\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")”了\n",
    "    \n",
    "所以可能因为 bert 不是针对检索优化的嵌入模型，而是一个理解模型。\n",
    "\n",
    "我决定找一找当今用来给LLM做RAG的开源嵌入模型。\n",
    "\n",
    "\n",
    "注意到 https://huggingface.co/BAAI/bge-base-zh-v1.5 被 Open WebUI 使用，很适合做RAG\n",
    "\n",
    "所以我直接替换为 \n",
    "\n",
    "```python\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-base-zh-v1.5\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-zh-v1.5\")\n",
    "```\n",
    "\n",
    "这一次回答几乎完全正确！至少问题关系没有理解错，信息可能少一些多一些，但都是从知识库知识回答的\n",
    "\n",
    "![alt text](image-12.png)\n",
    "\n",
    "![alt text](image-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b372b3",
   "metadata": {},
   "source": [
    "![alt text](image-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a7c51",
   "metadata": {},
   "source": [
    "可以看到我们选择的模型显著优于BERT！！我们的改进非常有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb00cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
