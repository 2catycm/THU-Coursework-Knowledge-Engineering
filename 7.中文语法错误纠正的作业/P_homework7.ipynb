{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 知识工程-作业7 中文语法错误纠正\n",
    "2024214500 叶璨铭\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 代码与文档格式说明\n",
    "\n",
    "> 本文档使用Jupyter Notebook编写，遵循Diátaxis 系统 Notebook实践 https://nbdev.fast.ai/tutorials/best_practices.html，所以同时包括了实验文档和实验代码。\n",
    "\n",
    "> 本次实验项目采用了类似于 Quarto + nbdev 的方法来同步Jupyter Notebook代码到python文件, 因而我们的实验文档导出为pdf和html格式可以进行阅读，而我们的代码也导出为python模块形式，可以作为代码库被其他项目使用。\n",
    "我们这样做的好处是，避免单独管理一堆 .py 文件，防止代码冗余和同步混乱，py文件和pdf文件都是从.ipynb文件导出的，可以保证实验文档和代码的一致性。\n",
    "\n",
    "> 本文档理论上支持多个格式，包括ipynb, html, docx, pdf, md 等，但是由于 quarto和nbdev 系统的一些bug，我们目前暂时只支持ipynb, docx, pdf文件，以后有空的时候解决bug可以构建一个[在线文档网站](https://thu-coursework-machine-learning-for-big-data-docs.vercel.app/)。您在阅读本文档时，可以选择您喜欢的格式来进行阅读，建议您使用 Visual Studio Code (或者其他支持jupyter notebook的IDE, 但是VSCode阅读体验最佳) 打开 `ipynb`格式的文档来进行阅读。\n",
    "\n",
    "\n",
    "> 为了记录我们自己修改了哪些地方，使用git进行版本控制，这样可以清晰地看出我们基于助教的代码在哪些位置进行了修改，有些修改是实现了要求的作业功能，而有些代码是对助教的代码进行了重构和优化。我将我在知识工程课程的代码，在作业截止DDL之后，开源到 https://github.com/2catycm/THU-Coursework-Knowledge-Engineering.git ，方便各位同学一起学习讨论。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## 代码规范说明\n",
    "\n",
    "在我们实现函数过程中，函数的docstring应当遵循fastai规范而不是numpy规范，这样简洁清晰，不会Repeat yourself。相应的哲学和具体区别可以看 \n",
    "https://nbdev.fast.ai/tutorials/best_practices.html#keep-docstrings-short-elaborate-in-separate-cells\n",
    "\n",
    "\n",
    "为了让代码清晰规范，在作业开始前，使用 `ruff format`格式化助教老师给的代码; \n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "\n",
    "哇！我们当场就检查出了代码错误！不只是格式化问题了，看看 metrics/m2scorer/Tokenizer.py:177:15 和 metrics/m2scorer/token_offsets.py:43:15 是怎么回事\n",
    "\n",
    "![alt text](image-2.png)\n",
    "\n",
    "![alt text](image-1.png)\n",
    "\n",
    "\n",
    "原来是 m2scorer 太老了，居然用了 Python2 的语法！我们简单修改为 `print(\"\")` 语法就可以了。\n",
    "不过语句有点多啊，我们一个个改有点不够优雅。\n",
    "\n",
    "Python官方有工具，\n",
    "```bash\n",
    "2to3 -w .\n",
    "```\n",
    "\n",
    "![alt text](image-3.png)\n",
    "\n",
    "改了特别多东西\n",
    "\n",
    "![alt text](image-4.png)\n",
    "\n",
    "终于勉强看起来正常了，不过细看代码还是有很多不正常的地方，看来学长只是想让我们参考代码，这个应该是难以跑通的。\n",
    "\n",
    "\n",
    "\n",
    "同时注意到VSCode-Pylance插件的报错\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431382e5",
   "metadata": {},
   "source": [
    "## 实验环境准备\n",
    "\n",
    "\n",
    "上次作业结束的时候，我们注意到我们想要尝试的最新方法只能支持3.12，PyTorch2.4也不够新，所以我们这次作业重新创建一个作业专属3.12环境。\n",
    "\n",
    "先装小依赖包, 然后安装最新pytorch\n",
    "\n",
    "```bash\n",
    "conda create -n assignments python=3.12\n",
    "conda activate assignments\n",
    "pip install -r ../requirements.txt\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "```\n",
    "\n",
    "注意到\n",
    "\n",
    "```python\n",
    "from elmoformanylangs import Embedder, logger\n",
    "import einops\n",
    "import jieba\n",
    "```\n",
    "\n",
    "参考 https://github.com/HIT-SCIR/ELMoForManyLangs， 安装\n",
    "\n",
    "```bash\n",
    "git submodule add https://github.com/HIT-SCIR/ELMoForManyLangs\n",
    "cd ELMoForManyLangs\n",
    "pip install -e .\n",
    "```\n",
    "PyTorch版本没有冲突，不用被乱装一通，太棒了。\n",
    "\n",
    "安装一下上次没有探究完的 RWKV \n",
    "```bash\n",
    "pip install -U git+https://github.com/TorchRWKV/flash-linear-attention\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## 原理回顾和课件复习\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "课上详细介绍了语法纠正任务的一些基本特点和难点，先介绍了规则模型和分类模型，然后开始介绍翻译任务用于语法纠正和噪声信道模型，然后详细介绍了 Seq2seq 的发展历程，最后提及了一下大模型方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "\n",
    "download.sh 的清华网盘链接过期了，还好学长给的压缩包已经处理好了数据 NLPCC 2018 Task 2，已经有“./data/processed/”\n",
    "\n",
    "```python\n",
    "train_dataset = GECDataset(\n",
    "    \"./data/processed/seg.train\", vocab_dict=vocab_dict, max_length=200\n",
    ")\n",
    "test_dataset = GECDataset(\n",
    "    \"./data/processed/seg.txt\", vocab_dict=vocab_dict, max_length=200\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af00a63",
   "metadata": {},
   "source": [
    "## 预训练模型下载\n",
    "\n",
    "根据main.py, 首先我们需要下载  \"zhs.model\"\n",
    "```python\n",
    "elmo_model = Embedder(\"zhs.model\", batch_size=16)\n",
    "vocab_dict = load_vocab_dict(\"./zhs.model/word.dic\")\n",
    "```\n",
    "\n",
    "根据 https://github.com/HIT-SCIR/ELMoForManyLangs\n",
    "有两个链接都是下载中文模型的，\n",
    "```bash\n",
    "wget http://vectors.nlpl.eu/repository/11/179.zip\n",
    "wget http://39.96.43.154/zhs.model.tar.bz2\n",
    "```\n",
    "后者链接失效了无法连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59636d0f",
   "metadata": {},
   "source": [
    "## 评价指标 Max Match 实现\n",
    "\n",
    "Max Match （因为有两个M 又叫作 M2 ）是由 NUS 的研究者在这篇2012 ACL 论文 https://aclanthology.org/N12-1067.pdf “Better Evaluation for Grammatical Error Correction” 提出的。\n",
    "\n",
    "老师的课件指出这是应用最广泛的指标之一。\n",
    "\n",
    "![alt text](image-5.png)\n",
    "\n",
    "\n",
    "MaxMatch (M²) 首先第一个思路是不再直接比较生成句子和参考句子的相似度，而是比较系统所做的“编辑操作” (edits) 与人类标注者提供的“标准编辑操作”的匹配程度，也就是课件中说的ei和gi。\n",
    "\n",
    "核心思想是“Max”，可以假设我们有多个标准答案，对于每一个人工标注的参考答案（即一个正确的句子版本），通过比较该参考答案和原始错误句子，找出从错误句子到这个特定正确版本所需的标准编辑操作。对于系统生成的句子所对应的一组编辑操作，M² 会尝试将其与每一个参考答案所对应的标准编辑操作集进行比较。\n",
    "\n",
    "它会计算系统编辑集与每一个标准编辑集之间的匹配程度（通常是计算重叠的编辑数量）。\n",
    "然后，它会选择那个能与系统编辑集产生最大匹配度（即最多重叠编辑）的参考答案。\n",
    "重要的是， M² 认为，只要系统做出的编辑与 任意一个 正确的参考答案中的编辑相匹配，这个编辑就是有效的。这就是“MaxMatch”的含义——在所有可能的标准答案中，找到对系统最有利（匹配度最高）的那一个来进行评估。\n",
    "\n",
    "M² 的主要目的是解决GEC评估中的模糊性问题：同一个错误修正问题可能有多种编辑操作序列导致相同的结果句子，而不同系统可能通过不同的编辑路径达到相同输出。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5fd9d0",
   "metadata": {},
   "source": [
    "那么具体要怎么实现呢？经过我的初步调查，助教建议的第一个参考代码 https://github.com/shibing624/pycorrector 虽然看起来集成了很多方法，但是 max match 指标似乎没有（或者不叫这个名字），评测的代码是 “https://github.com/shibing624/pycorrector/blob/master/pycorrector/utils/evaluate_utils.py” 但是我还没有看懂，这里面引入了\"SIGHAN\", \"句级评估结果\", \"设定需要纠错为正样本，无需纠错为负样本\" 这些概念，好像用了对比学习，但是没有直接说是用 Max Match。\n",
    "\n",
    "助教给我们的第二个代码就是 M² 的官方代码，也就是刚才我费了半天劲升级为 Python3 的代码。根据助教给的提示，我们可以直接外部调用其功能用来评估。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def maxmatch_metric(prediction_file: str # a file containing predicted output\n",
    "                    , label_file: str # a file containig groundtruth output\n",
    "                    ) -> Any:\n",
    "    \"\"\"\n",
    "    calculate maxmatch metrics\n",
    "\n",
    "    File content example\n",
    "    # prediction file\n",
    "    ```\n",
    "    冬 阴功 是 泰国 最 著名 的 菜 之一 ， 它 虽然 不 是 很 豪华 ， 但 它 的 味 确实 让 人 上瘾 ， 做法 也 不 难 、 不 复杂 。\n",
    "    首先 ， 我们 得 准备 : 大 虾六 到 九 只 、 盐 一 茶匙 、 已 搾 好 的 柠檬汁 三 汤匙 、 泰国 柠檬 叶三叶 、 柠檬 香草 一 根 、 鱼酱 两 汤匙 、 辣椒 6 粒 ， 纯净 水 4量杯 、 香菜 半量杯 和 草菇 10 个 。\n",
    "    ```\n",
    "    # label_file\n",
    "    ```\n",
    "    S 冬 阴功 是 泰国 最 著名 的 菜 之一 ， 它 虽然 不 是 很 豪华 ， 但 它 的 味 确实 让 人 上瘾 ， 做法 也 不 难 、 不 复杂 。\n",
    "    A 9 11|||W|||虽然 它|||REQUIRED|||-NONE-|||0\n",
    "\n",
    "    S 首先 ， 我们 得 准备 : 大 虾六 到 九 只 、 盐 一 茶匙 、 已 搾 好 的 柠檬汁 三 汤匙 、 泰国 柠檬 叶三叶 、 柠檬 香草 一 根 、 鱼酱 两 汤匙 、 辣椒 6 粒 ， 纯净 水 4量杯 、 香菜 半量杯 和 草菇 10 个 。\n",
    "    A 17 18|||S|||榨|||REQUIRED|||-NONE-|||0\n",
    "    A 38 39|||S|||六|||REQUIRED|||-NONE-|||0\n",
    "    A 43 44|||S|||四 量杯|||REQUIRED|||-NONE-|||0\n",
    "    A 49 50|||S|||十|||REQUIRED|||-NONE-|||0\n",
    "    ```\n",
    "    \"\"\"\n",
    "    subprocess.check_call(\n",
    "        [\"python\", \"metrics/m2scorer/m2scorer.py\", prediction_file, label_file]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22d676",
   "metadata": {},
   "source": [
    "为此，我特意把助教注释里面的文件内容拿了出来进行测试，看看是否评测正确。\n",
    "\n",
    "![alt text](image-6.png)\n",
    "\n",
    "确实得是0， prediction_file 没有改错，这个是啥都不干的语法修正器。\n",
    "\n",
    "然后我按照grond truth的要求，让LLM遵循ground truth的命令更正语法，得到 prediction_file2\n",
    "\n",
    "![alt text](image-7.png)\n",
    "\n",
    "没想到居然不是完全对，precision差一个，意思是我们不小心多改了一个什么东西。\n",
    "\n",
    "仔细检查了半天，发现原来是助教给我们的Python 注释有问题，冬阴功分词给合起来了，我们修改一下 Python 注释以及 predicition_file_correct.txt 就行。\n",
    "\n",
    "![alt text](image-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33c011",
   "metadata": {},
   "source": [
    "我们还可以写正则表达式提取一下这三个数值出来，这样返回的时候更好处理\n",
    "\n",
    "![alt text](image-9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f67843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxmatch_metric(prediction_file: str # a file containing predicted output\n",
    "                    , label_file: str # a file containig groundtruth output\n",
    "                    , verbose:bool = True\n",
    "                    ) -> Any:\n",
    "    \"\"\"\n",
    "    calculate maxmatch metrics\n",
    "\n",
    "    File content example\n",
    "    # prediction file\n",
    "    ```\n",
    "    冬 阴功 是 泰国 最 著名 的 菜 之一 ， 它 虽然 不 是 很 豪华 ， 但 它 的 味 确实 让 人 上瘾 ， 做法 也 不 难 、 不 复杂 。\n",
    "    首先 ， 我们 得 准备 : 大 虾六 到 九 只 、 盐 一 茶匙 、 已 搾 好 的 柠檬汁 三 汤匙 、 泰国 柠檬 叶三叶 、 柠檬 香草 一 根 、 鱼酱 两 汤匙 、 辣椒 6 粒 ， 纯净 水 4量杯 、 香菜 半量杯 和 草菇 10 个 。\n",
    "    ```\n",
    "    # label_file\n",
    "    ```\n",
    "    S 冬 阴功 是 泰国 最 著名 的 菜 之一 ， 它 虽然 不 是 很 豪华 ， 但 它 的 味 确实 让 人 上瘾 ， 做法 也 不 难 、 不 复杂 。\n",
    "    A 9 11|||W|||虽然 它|||REQUIRED|||-NONE-|||0\n",
    "\n",
    "    S 首先 ， 我们 得 准备 : 大 虾六 到 九 只 、 盐 一 茶匙 、 已 搾 好 的 柠檬汁 三 汤匙 、 泰国 柠檬 叶三叶 、 柠檬 香草 一 根 、 鱼酱 两 汤匙 、 辣椒 6 粒 ， 纯净 水 4量杯 、 香菜 半量杯 和 草菇 10 个 。\n",
    "    A 17 18|||S|||榨|||REQUIRED|||-NONE-|||0\n",
    "    A 38 39|||S|||六|||REQUIRED|||-NONE-|||0\n",
    "    A 43 44|||S|||四 量杯|||REQUIRED|||-NONE-|||0\n",
    "    A 49 50|||S|||十|||REQUIRED|||-NONE-|||0\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # subprocess.check_call(\n",
    "    #     [\"python\", \"metrics/m2scorer/m2scorer.py\", prediction_file, label_file]\n",
    "    # )\n",
    "    # 执行命令并捕获输出\n",
    "    result = subprocess.check_output(\n",
    "        [\"python\", \"metrics/m2scorer/m2scorer.py\", prediction_file, label_file],\n",
    "        text=True  # 直接获取文本输出，无需解码\n",
    "    )\n",
    "\n",
    "    # 打印原始输出\n",
    "    if verbose:\n",
    "        print(\"m2scorer评测中:\")\n",
    "        print(result)\n",
    "\n",
    "    # 使用正则表达式提取指标\n",
    "    metrics = {}\n",
    "    metrics_pattern = re.compile(r'(Precision|Recall|F0\\.5)\\s*:\\s*([\\d\\.]+)')\n",
    "    matches = metrics_pattern.findall(result)\n",
    "    \n",
    "    # 将匹配的结果转换为字典\n",
    "    for key, value in matches:\n",
    "        metrics[key] = float(value)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bbcf2",
   "metadata": {},
   "source": [
    "当然这样会被底层代码限制住，只能获得4位小数，虽然对我们这次实验够用了，但是为了避免后人再被这个m2scorer坑住，我们决定把里面的代码改造一些出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440eb5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "this_file = Path(__file__).resolve()\n",
    "this_directory = this_file.parent\n",
    "import sys\n",
    "sys.path.append((this_directory/\"m2scorer\").as_posix())\n",
    "\n",
    "import levenshtein as levenshtein\n",
    "from util import paragraphs\n",
    "from util import smart_open\n",
    "from typing import Any\n",
    "\n",
    "def load_annotation(gold_file):\n",
    "    source_sentences = []\n",
    "    gold_edits = []\n",
    "    fgold = smart_open(gold_file, \"r\")\n",
    "    puffer = fgold.read()\n",
    "    fgold.close()\n",
    "    # puffer = puffer.decode('utf8')\n",
    "    for item in paragraphs(puffer.splitlines(True)):\n",
    "        item = item.splitlines(False)\n",
    "        sentence = [line[2:].strip() for line in item if line.startswith(\"S \")]\n",
    "        assert sentence != []\n",
    "        annotations = {}\n",
    "        for line in item[1:]:\n",
    "            if line.startswith(\"I \") or line.startswith(\"S \"):\n",
    "                continue\n",
    "            assert line.startswith(\"A \")\n",
    "            line = line[2:]\n",
    "            fields = line.split(\"|||\")\n",
    "            start_offset = int(fields[0].split()[0])\n",
    "            end_offset = int(fields[0].split()[1])\n",
    "            etype = fields[1]\n",
    "            if etype == \"noop\":\n",
    "                start_offset = -1\n",
    "                end_offset = -1\n",
    "            corrections = [\n",
    "                c.strip() if c != \"-NONE-\" else \"\" for c in fields[2].split(\"||\")\n",
    "            ]\n",
    "            # NOTE: start and end are *token* offsets\n",
    "            original = \" \".join(\" \".join(sentence).split()[start_offset:end_offset])\n",
    "            annotator = int(fields[5])\n",
    "            if annotator not in list(annotations.keys()):\n",
    "                annotations[annotator] = []\n",
    "            annotations[annotator].append(\n",
    "                (start_offset, end_offset, original, corrections)\n",
    "            )\n",
    "        tok_offset = 0\n",
    "        for this_sentence in sentence:\n",
    "            tok_offset += len(this_sentence.split())\n",
    "            source_sentences.append(this_sentence)\n",
    "            this_edits = {}\n",
    "            for annotator, annotation in annotations.items():\n",
    "                this_edits[annotator] = [\n",
    "                    edit\n",
    "                    for edit in annotation\n",
    "                    if edit[0] <= tok_offset\n",
    "                    and edit[1] <= tok_offset\n",
    "                    and edit[0] >= 0\n",
    "                    and edit[1] >= 0\n",
    "                ]\n",
    "            if len(this_edits) == 0:\n",
    "                this_edits[0] = []\n",
    "            gold_edits.append(this_edits)\n",
    "    return (source_sentences, gold_edits)\n",
    "\n",
    "\n",
    "def maxmatch_metric(prediction_file: str # a file containing predicted output\n",
    "                    , label_file: str # a file containig groundtruth output\n",
    "                    , verbose:bool = True\n",
    "                    ) -> Any:\n",
    "    \"\"\"\n",
    "    calculate maxmatch metrics\n",
    "\n",
    "    File content example\n",
    "    # prediction file\n",
    "    ```\n",
    "    冬 阴功 是 泰国 最 著名 的 菜 之一 ， 它 虽然 不 是 很 豪华 ， 但 它 的 味 确实 让 人 上瘾 ， 做法 也 不 难 、 不 复杂 。\n",
    "    首先 ， 我们 得 准备 : 大 虾六 到 九 只 、 盐 一 茶匙 、 已 搾 好 的 柠檬汁 三 汤匙 、 泰国 柠檬 叶三叶 、 柠檬 香草 一 根 、 鱼酱 两 汤匙 、 辣椒 6 粒 ， 纯净 水 4量杯 、 香菜 半量杯 和 草菇 10 个 。\n",
    "    ```\n",
    "    # label_file\n",
    "    ```\n",
    "    S 冬 阴功 是 泰国 最 著名 的 菜 之一 ， 它 虽然 不 是 很 豪华 ， 但 它 的 味 确实 让 人 上瘾 ， 做法 也 不 难 、 不 复杂 。\n",
    "    A 9 11|||W|||虽然 它|||REQUIRED|||-NONE-|||0\n",
    "\n",
    "    S 首先 ， 我们 得 准备 : 大 虾六 到 九 只 、 盐 一 茶匙 、 已 搾 好 的 柠檬汁 三 汤匙 、 泰国 柠檬 叶三叶 、 柠檬 香草 一 根 、 鱼酱 两 汤匙 、 辣椒 6 粒 ， 纯净 水 4量杯 、 香菜 半量杯 和 草菇 10 个 。\n",
    "    A 17 18|||S|||榨|||REQUIRED|||-NONE-|||0\n",
    "    A 38 39|||S|||六|||REQUIRED|||-NONE-|||0\n",
    "    A 43 44|||S|||四 量杯|||REQUIRED|||-NONE-|||0\n",
    "    A 49 50|||S|||十|||REQUIRED|||-NONE-|||0\n",
    "    ```\n",
    "    \"\"\"\n",
    "    max_unchanged_words = 2\n",
    "    beta = 0.5\n",
    "    ignore_whitespace_casing = False\n",
    "    very_verbose = False\n",
    "\n",
    "    # load source sentences and gold edits\n",
    "    source_sentences, gold_edits = load_annotation(label_file)\n",
    "\n",
    "    # load system hypotheses\n",
    "    fin = smart_open(prediction_file, \"r\")\n",
    "    system_sentences = [line.strip() for line in fin.readlines()]\n",
    "    fin.close()\n",
    "\n",
    "    p, r, f1 = levenshtein.batch_multi_pre_rec_f1(\n",
    "        system_sentences,\n",
    "        source_sentences,\n",
    "        gold_edits,\n",
    "        max_unchanged_words,\n",
    "        beta,\n",
    "        ignore_whitespace_casing,\n",
    "        verbose,\n",
    "        very_verbose,\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"Precision\": p,\n",
    "        \"Recall\": r,\n",
    "        \"F_{}\".format(beta): f1\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# 如果需要测试函数，可以调用它并打印结果\n",
    "if __name__ == \"__main__\":\n",
    "    prediction_file = (this_directory/\"../data/test_prediction_file_correct.txt\").as_posix()\n",
    "    label_file = (this_directory/\"../data/test_label_file.txt\").as_posix()\n",
    "    metrics = maxmatch_metric(prediction_file, label_file)\n",
    "    print(\"Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5839105a",
   "metadata": {},
   "source": [
    "![alt text](image-10.png)\n",
    "\n",
    "看来实现是正确的，而且我们还能看到详细的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## 数据加载\n",
    "\n",
    "\n",
    "\n",
    "我们看下 main.py 文件\n",
    "\n",
    "使用到数据的地方是\n",
    "\n",
    "```python\n",
    "from data_util import MyDataset, collate_fn\n",
    "...\n",
    "MyDataset(\n",
    "        \"./data/train.tsv\",\n",
    "        max_length=max_length,\n",
    "        train=True,\n",
    "        max_example_num=max_train_example,\n",
    "    ),\n",
    "collate_fn=functools.partial(collate_fn, device=device),\n",
    "```\n",
    "\n",
    "助教已经帮我们实现了 collate_fn 关键是我们要写 MyDataset \n",
    "\n",
    "首先注意到 MyDataset[i] 返回的是一个元组，包含文本和标签\n",
    "\n",
    "```python\n",
    "def __getitem__(self, item):\n",
    "    return self.text[item], self.label[item]\n",
    "```\n",
    "\n",
    "需要自己有list来存。现在可以写 load \n",
    "\n",
    "我们需要观察一下数据格式\n",
    "\n",
    "![alt text](image-2.png)\n",
    "\n",
    "train.tsv 和 dev.tsv 都是  “sentence label”，test.tsv 是 “index sentence”， 这次作业 main.py 只要求我们做 train 和 dev的,c传递的参数都是 train=True。\n",
    "\n",
    "注意 csv 是 \"comma separated values\" 的缩写，tsv 是 \"tab separated values\" 的缩写，csv文件的分隔符是逗号，而tsv文件的分隔符是制表符（tab），所以我们需要使用 `sep=\"\\t\"` 来读取数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "\n",
    "\n",
    "def load(\n",
    "    self,\n",
    "    file: str,  # file path\n",
    "    train: bool = True,  # whether is training file\n",
    ") -> Tuple[List[List[str]], List[int]]:  # Returns (text, label), text input and label\n",
    "    \"\"\"\n",
    "    load file into texts and labels\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # 使用pandas读取文件，自动推断分隔符\n",
    "    try:\n",
    "        # 首先尝试tab分隔符，因为这是期望的格式\n",
    "        df = pd.read_csv(file, sep=\"\\t\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file with tab separator: {e}\")\n",
    "        print(\"Trying to read with auto-detected separator...\")\n",
    "        # 如果失败，让pandas尝试自动推断分隔符\n",
    "        df = pd.read_csv(file, sep=None, engine=\"python\")\n",
    "\n",
    "    text = df[\"sentence\"].astype(str).tolist()\n",
    "    # 分词\n",
    "    text = [sentence.split() for sentence in text]\n",
    "\n",
    "    if train:\n",
    "        # 训练集格式: sentence  label\n",
    "        label = df[\"label\"].astype(int).tolist()\n",
    "    else:\n",
    "        # 测试集可能没有标签，默认为 -1 表示不知道\n",
    "        label = [-1] * len(text)\n",
    "\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727e7c0",
   "metadata": {},
   "source": [
    "注意助教对  Returns (text, label), text input and label 的类型标注有误。\n",
    "\n",
    "首先label应该是 List[int] 类型，参考官方文档 https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html ，可以是float，但是不应该是str类型。\n",
    "\n",
    "其次text不应该是 List[str], 这里我们需要查看allennlp的 batch_to_ids 函数的约定 https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py ，可以看到其要求的输入是 List[List[str]], 也就是说需要对句子进行分词！\n",
    "\n",
    "\n",
    "![](image-3.png)\n",
    "\n",
    "我们差点就被原本的注释带偏啦，还好检查了allennlp的文档。\n",
    "\n",
    "\n",
    "分词并不难，因为这次作业是 “英文评论情感分类”，可以直接按照空白符分词，使用 `str.split()` 就可以了，不需要上次那样用结巴分词。\n",
    "\n",
    "当然，如果做得细致些，应该用 elmo 的 tokenier 去做分词，或者用nltk。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daba44c",
   "metadata": {},
   "source": [
    "现在可以实现 pad 函数\n",
    "\n",
    "事实上，刚才我们看了源码知道，allennlp的elmo已经实现了padding，会根据句子和单词的最大长度来补齐，实际上我们不应该做任何操作！\n",
    "\n",
    "当然，助教给了我们一个 max_length 的参数，实际上是用来限制句子长度的，超过这个长度的句子会被截断, 如果比elmo从数据发现的最大长度还要长，那多补一些也无妨，我们还是能实现一个。\n",
    "\n",
    "不过最重要的问题是，pad token是什么？上一次作业是助教定义的词库，传入了pad和unknown的id，这次作业我们需要遵循allennlp的elmo的约定！\n",
    "\n",
    "这下我们不得不继续查看源码 https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py\n",
    "\n",
    "![](image-4.png)\n",
    "\n",
    "这下可以确认pad token是0，从而实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44428b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(\n",
    "    self,\n",
    "    text_ids: torch.Tensor,  # size N*L*D\n",
    ") -> torch.Tensor:  # Returns padded_text_id, size N*max_length*D\n",
    "    \"\"\"\n",
    "    pad text_ids to max_length\n",
    "    \"\"\"\n",
    "    N, L, D = text_ids.shape\n",
    "    if L >= self.max_length:\n",
    "        # 如果文本长度大于等于最大长度，截断\n",
    "        return text_ids[:, : self.max_length, :]\n",
    "    else:\n",
    "        # 如果文本长度小于最大长度，填充\n",
    "        padding = torch.zeros(\n",
    "            N, self.max_length - L, D, dtype=text_ids.dtype, device=text_ids.device\n",
    "        )\n",
    "        padded_text_ids = torch.cat([text_ids, padding], dim=1)\n",
    "        return padded_text_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118365f2",
   "metadata": {},
   "source": [
    "测试一下, 用 https://github.com/google-deepmind/treescope 看清楚数据是什么样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a33c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import treescope\n",
    "\n",
    "treescope.basic_interactive_setup(autovisualize_arrays=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab11814",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(\"./data/dev.tsv\")\n",
    "t, l = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7602eeb",
   "metadata": {},
   "source": [
    "## CNN 神经网络实现\n",
    "\n",
    "这次的TextCNN和第四次作业的基本一样，只是embedding换成了allennlp的elmo representation，然后分类数量换成了2，其他的都一样。\n",
    "\n",
    "上次我写得代码已经比较优雅高效，更多关于这段代码的理解和实现逻辑参阅 上次作业报告 https://github.com/2catycm/THU-Coursework-Knowledge-Engineering/blob/master/4.%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%9A%84%E4%BD%9C%E4%B8%9A/P_homework4.ipynb \n",
    "\n",
    "这次我们增加一个dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        options_file: str,  # elmo file\n",
    "        weight_file: str,  # elmo weight file\n",
    "        vector_size: int,  # word embedding dim\n",
    "        filter_size: List[int] = [2, 3, 4, 5],  # kernel size for each layer of CNN\n",
    "        channels: int = 64,  # output channel for CNN\n",
    "        max_length: int = 1024,  # max length of input sentence\n",
    "        dropout=0.5,  # dropout rate\n",
    "    ):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "        ####################\n",
    "        # 初始化嵌入层已经通过Elmo完成\n",
    "        # 直接用上次作业的代码\n",
    "        # Build a stack of 1D CNN layers for each filter size\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(in_channels=vector_size, out_channels=channels, kernel_size=k)\n",
    "                # Conv1dViaConv2d(\n",
    "                #     in_channels=vector_size,\n",
    "                #     out_channels=channels,\n",
    "                #     kernel_size=k,\n",
    "                #     conv_2d=KAN_Convolutional_Layer,\n",
    "                # )\n",
    "                for k in filter_size\n",
    "            ]\n",
    "        )\n",
    "        # Final linear layer for label prediction; number of classes equals len(label2index)\n",
    "        # CNN的输出通道数 × 不同卷积核的数量\n",
    "        self.linear = nn.Linear(\n",
    "            channels * len(filter_size), 2\n",
    "        )  # 二分类问题（正面/负面）\n",
    "        #  Dropout层，防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: torch.Tensor,  # input sentence, size N*L\n",
    "    ) -> torch.Tensor:  # predicted_logits: torch.tensor of size N*C (number of classes)\n",
    "        # 获取ELMo嵌入表示\n",
    "        inputs = self.embedding(inputs)[\"elmo_representations\"][\n",
    "            0\n",
    "        ]  # [N, L, vector_size]\n",
    "\n",
    "        # Convolutional layer\n",
    "        x = inputs.transpose(1, 2)  # 卷积需要将词向量维度放在最后 (N*D*L)\n",
    "        x = [conv(x) for conv in self.convs]\n",
    "        x = [nn.functional.gelu(i) for i in x]  # 每一个 i是 (N*C*Li) ， Li = L - ki + 1\n",
    "        # Pooling layer\n",
    "        x = [\n",
    "            nn.functional.max_pool1d(\n",
    "                i,\n",
    "                kernel_size=i.size(2),  # 对 Li 去做 max_pooling\n",
    "            ).squeeze(2)\n",
    "            for i in x  # 每一个 i是 (N*C*Li)\n",
    "        ]  # 每一个 item 变为 (N*C)\n",
    "        # Concatenate all pooling results\n",
    "        x = torch.cat(x, dim=1)  # 把每一个 item 拼接起来，变为 (N, C*len(filter_size))\n",
    "        # 应用dropout\n",
    "        x = self.dropout(x)\n",
    "        # Linear layer\n",
    "        x = self.linear(x)  # 分类，得到 (N*K)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a20f1",
   "metadata": {},
   "source": [
    "其中我们还用了 Conv2d兼容层，来尝试实现 KAN Conv，但是并没有成功，这次老师讲解的重点是RNN，所以今天我们不过多探索Kolmogorov–Arnold Networks。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc0eef",
   "metadata": {},
   "source": [
    "## RNN 神经网络实现\n",
    "\n",
    "### LSTM 实现\n",
    "\n",
    "PyTorch 官方已经实现LSTM块，直接调用就是最佳实践，优雅高效。 https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "我们只需要确保接口和TextCNN一致就可以了，为了公平比较，embedding都是用elmo一样的设置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da293193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from allennlp.modules.elmo import Elmo\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        options_file: str,  # elmo options file\n",
    "        weight_file: str,  # elmo weight file\n",
    "        vector_size: int,  # word embedding dim\n",
    "        filter_size: List[int] = [2, 3, 4, 5],  # 保留接口，与CNN一致，但不使用\n",
    "        channels: int = 64,  # 作为LSTM隐藏层维度\n",
    "        max_length: int = 1024,  # 最大句子长度\n",
    "        dropout: float = 0.5,  # dropout rate\n",
    "    ):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        self.embedding = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "        # 使用LSTM进行特征抽取，使用channels作为隐藏层维度\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=vector_size, hidden_size=channels, batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 最后全连接分类层\n",
    "        self.linear = nn.Linear(channels, 2)  # 二分类问题\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        利用ELMo嵌入和LSTM进行前向传播\n",
    "        \"\"\"\n",
    "        # 获取ELMo嵌入表示, 输出形状为 (N, L, vector_size)\n",
    "        x = self.embedding(inputs)[\"elmo_representations\"][0]\n",
    "        # 通过LSTM，输出h_n形状为 (num_layers, N, hidden_size)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # 提取最后一层隐藏状态，形状为 (N, hidden_size)\n",
    "        h = h_n[-1]\n",
    "        # 应用Dropout\n",
    "        h = self.dropout(h)\n",
    "        # 分类\n",
    "        out = self.linear(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daff695",
   "metadata": {},
   "source": [
    "其中对于LSTM的参数，batch_first 是因为我们elmo得到的顺序是 (N, L, vector_size)，N在前面。\n",
    "\n",
    "input_size 和 hidden_size 有所不同，类似于CNN中的in_channels 和 out_channels。 \n",
    "\n",
    "\n",
    "\n",
    "在forward中用的时候，输入 input, (h_0, c_0)\n",
    "输出 output, (h_n, c_n)\n",
    "\n",
    "h_0 和 c_0 可以不提供 ，“Defaults to zeros”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e21590",
   "metadata": {},
   "source": [
    "### RWKV 实现\n",
    "\n",
    "RWKV 是 目前较为先进的RNN模型，号称结合了 Attention 并行训练和长文本建模的能力和RNN高效推理无限上下文长度的能力。RWKV 本质上和LSTM一样都是RNN模型，只是其使用了不同的激活函数和门控机制以及其他的一些高级操作。\n",
    "\n",
    "感觉RWKV的开源做的很好，在linux基金会下 https://rwkv.cn/news/read?id=15 ，看起来很有前景。\n",
    "\n",
    "RWKV-LM 的开源代码很复杂，有cuda kernel c++啥的，很难改。通过和RWKV社区成员沟通交流，发现这个https://github.com/TorchRWKV/flash-linear-attention/tree/stable 实现比较优雅，用torch写，但是用triton编译。\n",
    "\n",
    "我们安装一下。\n",
    "\n",
    "```bash\n",
    "pip install -U git+https://github.com/TorchRWKV/flash-linear-attention\n",
    "```\n",
    "\n",
    "看源码 https://github.com/TorchRWKV/flash-linear-attention/blob/stable/fla/layers/rwkv7.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from allennlp.modules.elmo import Elmo\n",
    "from typing import List\n",
    "\n",
    "# 引入 RWKV7Attention 模块\n",
    "from fla.layers.rwkv7 import RWKV7Attention\n",
    "\n",
    "\n",
    "class TextRWKV(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        options_file: str,  # elmo选项文件\n",
    "        weight_file: str,  # elmo权重文件\n",
    "        vector_size: int,  # 词向量维度\n",
    "        filter_size: List[int] = [2, 3, 4, 5],  # 保持接口一致，但不使用\n",
    "        channels: int = 64,  # 保持接口一致，可用于其他用途\n",
    "        max_length: int = 1024,  # 最大句子长度\n",
    "        dropout: float = 0.5,  # dropout 概率\n",
    "    ):\n",
    "        super(TextRWKV, self).__init__()\n",
    "        # 使用ELMo构造嵌入层\n",
    "        self.embedding = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "        # 构造 RWKV 模块，采用 chunk 模式，hidden_size 使用 vector_size\n",
    "        self.rwkv = RWKV7Attention(mode=\"chunk\", hidden_size=vector_size, head_dim=64)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 最后全连接分类层，将特征映射到二分类问题\n",
    "        self.linear = nn.Linear(vector_size, 2)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        利用ELMo嵌入和RWKV7Attention模块进行前向传播\n",
    "\n",
    "        Parameters:\n",
    "            inputs: torch.Tensor\n",
    "                输入句子（形状为 N x L）\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 分类预测 logits（形状为 N x 2）\n",
    "        \"\"\"\n",
    "        # 获取ELMo嵌入表示，形状为 (N, L, vector_size)\n",
    "        x = self.embedding(inputs)[\"elmo_representations\"][0]\n",
    "        # 通过RWKV模块，输出形状假设为 (N, L, vector_size)\n",
    "        o, _, _, _ = self.rwkv(x)\n",
    "        # 对时间步进行最大池化，得到 (N, vector_size) 表示\n",
    "        h, _ = torch.max(o, dim=1)\n",
    "        # 应用ReLU激活增强非线性，然后dropout\n",
    "        h = torch.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        # 全连接分类层，输出二分类 logits\n",
    "        out = self.linear(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa0390",
   "metadata": {},
   "source": [
    "## 运行效果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516992bd",
   "metadata": {},
   "source": [
    "首先我们直接运行一下CNN，为了让速度快一些，我改了batch size为16\\*64，learning rate也\\*16。\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=1 python main.py\n",
    "```\n",
    "\n",
    "![](image-5.png)\n",
    "\n",
    "可以看到效果一般，只到80。\n",
    "\n",
    "我们把batch size换回 64 再跑一次\n",
    "\n",
    "\n",
    "在跑之前，我们修改一下 main.py 代码，第一，老规矩啦，要支持argparse；第二这一次助教用了tqdm，但是中间完全没有反馈，根本看不到网络训练地好不好，不知道刚才80的问题在哪，所以我们加上loss和acc的反馈\n",
    "\n",
    "```python\n",
    "out_bar = tqdm(range(total_epoch))\n",
    "for epoch in out_bar: \n",
    "    ...\n",
    "    bar = tqdm(train_loader)\n",
    "    for text, label in bar:\n",
    "        ...\n",
    "        bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "    ...\n",
    "    out_bar.set_description(f\"Epoch: {epoch} Max Accuracy: {max_acc:.4f}\")\n",
    "```\n",
    "\n",
    "好现在跑\n",
    "\n",
    "![](image-6.png)\n",
    "\n",
    "可以看到达到了 85.89，四舍五入勉强复现了助教说的 86， 那么问题有可能是dropout=0.5 太大。\n",
    "\n",
    "现在关闭dropout，看看效果\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=1 python main.py --model cnn --batch_size 64 --dropout 0\n",
    "```\n",
    "\n",
    "![alt text](image-7.png)\n",
    "\n",
    "得到85.21\n",
    "看来还是有0.5的dropout性能更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc3a51",
   "metadata": {},
   "source": [
    "### LSTM 的性能\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=6 python main.py --model lstm --batch_size 64 --dropout 0.5\n",
    "```\n",
    "\n",
    "![alt text](image-8.png)\n",
    "\n",
    "效果大幅弱于 CNN！\n",
    "\n",
    "关闭dropout后性能更弱了\n",
    "\n",
    "![alt text](image-9.png)\n",
    "\n",
    "\n",
    "仔细检查刚才上面的代码，我发现，相比于CNN，我们没有增加RELU函数。虽然根据课件，RNN自己有激活函数，下一个状态的hidden state是从上一个状态得到的，有激活函数。\n",
    "\n",
    "但是为了TextCNN差不多，我们还是加上RELU函数吧，看看效果\n",
    "\n",
    "同时把LSTM变成双向LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371cf72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        options_file: str,  # elmo options file\n",
    "        weight_file: str,  # elmo weight file\n",
    "        vector_size: int,  # word embedding dim\n",
    "        filter_size: List[int] = [2, 3, 4, 5],  # 保留接口，与CNN一致，但不使用\n",
    "        channels: int = 64,  # 作为LSTM隐藏层维度\n",
    "        max_length: int = 1024,  # 最大句子长度\n",
    "        dropout: float = 0.5,  # dropout rate\n",
    "    ):\n",
    "        super(TextLSTM, self).__init__()\n",
    "        self.embedding = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "        # 使用LSTM进行特征抽取，使用channels作为隐藏层维度\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=vector_size,\n",
    "            hidden_size=channels,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 最后全连接分类层\n",
    "        self.linear = nn.Linear(2 * channels, 2)  # 二分类问题，双向LSTM输出\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        利用ELMo嵌入和LSTM进行前向传播\n",
    "        \"\"\"\n",
    "        # 获取ELMo嵌入表示, 输出形状为 (N, L, vector_size)\n",
    "        x = self.embedding(inputs)[\"elmo_representations\"][0]\n",
    "        # 通过LSTM，输出h_n形状为 (num_layers, N, hidden_size)\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        # 双向LSTM，h_n形状为 (num_directions, N, hidden_size)，拼接正反向的最后隐藏状态\n",
    "        h = torch.cat([h_n[0], h_n[1]], dim=1)\n",
    "        # 添加ReLU激活，增强非线性能力\n",
    "        h = torch.relu(h)\n",
    "        # 应用Dropout\n",
    "        h = self.dropout(h)\n",
    "        # 分类\n",
    "        out = self.linear(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d9df0",
   "metadata": {},
   "source": [
    "![alt text](image-10.png)\n",
    "\n",
    "这下效果终于正常了。\n",
    "\n",
    "在老师的课件中\n",
    "![alt text](image-11.png)\n",
    "\n",
    "有两种结构，我们用的是第二种，而且只有单层。\n",
    "\n",
    "![alt text](image-12.png)\n",
    "\n",
    "老师还提到CNN优于RNN也是正常的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe72c5",
   "metadata": {},
   "source": [
    "### RWKV 的性能\n",
    "\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=1 PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python main.py --model rwkv\n",
    "\n",
    "遇到报错\n",
    "```bash\n",
    "ImportError: cannot import name 'DeviceMesh' from 'torch.distributed.tensor' \n",
    "```\n",
    "\n",
    "https://github.com/allenai/OLMo/issues/559\n",
    "\n",
    "但是我们按照issue把PyTorch升级到最新，也还是报这个错。\n",
    "\n",
    "看来RWKV不是那么容易跑啊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93abca13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f56d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
