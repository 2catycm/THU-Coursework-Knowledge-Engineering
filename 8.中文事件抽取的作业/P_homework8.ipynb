{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 知识工程-作业8 中文事件抽取\n",
    "2024214500 叶璨铭\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 代码与文档格式说明\n",
    "\n",
    "> 本文档使用Jupyter Notebook编写，遵循Diátaxis 系统 Notebook实践 https://nbdev.fast.ai/tutorials/best_practices.html，所以同时包括了实验文档和实验代码。\n",
    "\n",
    "> 本文档理论上支持多个格式，包括ipynb, docx, pdf 等。您在阅读本文档时，可以选择您喜欢的格式来进行阅读，建议您使用 Visual Studio Code (或者其他支持jupyter notebook的IDE, 但是VSCode阅读体验最佳) 打开 `ipynb`格式的文档来进行阅读。\n",
    "\n",
    "> 为了记录我们自己修改了哪些地方，使用git进行版本控制，这样可以清晰地看出我们基于助教的代码在哪些位置进行了修改，有些修改是实现了要求的作业功能，而有些代码是对原本代码进行了重构和优化。我将我在知识工程课程的代码，在作业截止DDL之后，开源到 https://github.com/2catycm/THU-Coursework-Knowledge-Engineering.git ，方便各位同学一起学习讨论。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## 代码规范说明\n",
    "\n",
    "在我们实现函数过程中，函数的docstring应当遵循fastai规范而不是numpy规范，这样简洁清晰，不会Repeat yourself。相应的哲学和具体区别可以看 \n",
    "https://nbdev.fast.ai/tutorials/best_practices.html#keep-docstrings-short-elaborate-in-separate-cells\n",
    "\n",
    "\n",
    "为了让代码清晰规范，在作业开始前，使用 `ruff format`格式化助教老师给的代码; \n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "\n",
    "很好，这次代码格式化没有报错。\n",
    "\n",
    "\n",
    "注意pylance报错\n",
    "\n",
    "![alt text](image-1.png)\n",
    "\n",
    "参考 https://github.com/huggingface/transformers/issues/15497\n",
    "\n",
    "![alt text](image-2.png)\n",
    "\n",
    "我们的更新。\n",
    "\n",
    "没区别，实际上就用torch的好了\n",
    "```python\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431382e5",
   "metadata": {},
   "source": [
    "## 实验环境准备\n",
    "\n",
    "采用上次的作业专属环境，为了跑通最新方法，使用3.12 和 torch 2.6\n",
    "\n",
    "```bash\n",
    "conda create -n assignments python=3.12\n",
    "conda activate assignments\n",
    "pip install -r ../requirements.txt\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "pip install -U git+https://github.com/TorchRWKV/flash-linear-attention\n",
    "```\n",
    "\n",
    "注意到\n",
    "\n",
    "```python\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "```\n",
    "\n",
    "参考 https://github.com/chakki-works/seqeval\n",
    "\n",
    "```bash\n",
    "pip install seqeval\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## 原理回顾和课件复习\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "课上详细介绍了事件抽取任务的一些基本特点和难点，事件肯定和时间有关系，是动态的知识，\n",
    "“清华大学在北京”就是相对静态的。“五月份清华举行校庆”那才有事件。实体图谱就没有事件图谱厉害。\n",
    "\n",
    "事件会有**触发词 trigger**，决定事件的类别；论元 argument，实体、事件、属性，应该有时间，没有事件就不是论元。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "\n",
    "download.sh 的清华网盘链接过期了，还好非常好的学长给的压缩包已经处理好了数据 raw。\n",
    "\n",
    "现在我们写 preprocess\n",
    "\n",
    "首先观察数据格式\n",
    "![alt text](image-3.png)\n",
    "\n",
    "这个实际上是jsonl，一行一个json\n",
    "\n",
    "每一行数据，有 id, text, labels, distant_trigger 四个字段\n",
    "id就是数据的id，text就是句子，distant_trigger 是 句子里面 trigger的列表。labels 则是多个对象，一个对象是有 trigger, object, subject, time 和 location。\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": 5009,\n",
    "    \"text\": \"晚年的毛岸青关心国家大事，关注祖国统一，拥护改革开放，热心支持老少边穷地区建设，多次和夫人邵华、儿子毛新宇重走长征路，到革命老区、到工厂、到农村调研，并以多种形式帮助失学儿童，支持创办了多个青少年爱国主义教育基地\",\n",
    "    \"labels\": [\n",
    "        {\n",
    "            \"trigger\": [\"支持创办\", 88],\n",
    "            \"object\": [\"儿子毛新宇\", 48],\n",
    "            \"subject\": [\"多个青少年爱国主义教育基地\", 93],\n",
    "            \"time\": \"\",\n",
    "            \"location\": \"\",\n",
    "        }\n",
    "    ],\n",
    "    \"distant_trigger\": [\"帮助\", \"支持\", \"拥护\", \"建设\", \"开放\", \"创办\", \"改革\", \"关注\"],\n",
    "}\n",
    "```\n",
    "\n",
    "为什么 labels里面的比 distant_trigger 的少呢？\n",
    "\n",
    "远距离触发词（Distant Trigger）是一组与事件相关的关键词或短语，它们可能不是直接的触发词，但可以提供上下文信息，帮助模型更全面地理解事件的语义。这些词通常是通过某种方法（如规则、统计分析或知识库）从文本中提取出来的，用于扩展触发词的语义范围。在标注数据不足的情况下，远距离触发词可以作为一种辅助信息。\n",
    "\n",
    "\n",
    "现在我们来写要求的函数 ，首先了解 IOB2 格式 https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging) 。\n",
    "\n",
    "IOB是 inside, outside, beginning 的缩写，也叫作BIO。\n",
    "\n",
    "IOB2 比较好理解，\n",
    "```bash\n",
    "Alex B-PER\n",
    "is O\n",
    "going O\n",
    "to O\n",
    "Los B-LOC\n",
    "Angeles I-LOC\n",
    "in O\n",
    "California B-LOC\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04fd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "def process_trigger_data(file):\n",
    "    \"\"\"\n",
    "    convert raw data into sequence-labeling format data for trigger identification, and save to `./data/processed/trigger`\n",
    "    each line in the converted contains `token[space]label`\n",
    "    you can use IOB2 format tagging https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging), or other tagging schema you think fit\n",
    "    use empty line to indicate end of one sentence\n",
    "    if using IOB2 format, the example output would be like\n",
    "    ```\n",
    "    本 O\n",
    "    平 O\n",
    "    台 O\n",
    "    S O\n",
    "    S O\n",
    "    R O\n",
    "    N O\n",
    "    发 B-EVENT\n",
    "    表 I-EVENT\n",
    "    了 O\n",
    "    题 O\n",
    "    为 O\n",
    "    《 O\n",
    "    夏 O\n",
    "    季 O\n",
    "    冠 O\n",
    "    状 O\n",
    "    病 O\n",
    "    毒 O\n",
    "    流 O\n",
    "    行 O\n",
    "    会 O\n",
    "    减 O\n",
    "    少 O\n",
    "    吗 O\n",
    "\n",
    "    对 O\n",
    "    于 O\n",
    "    该 O\n",
    "    举 O\n",
    "    动 O\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    with open(f\"./data/raw/{file}.json\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    outlines = []\n",
    "    ##################\n",
    "    bar = tqdm(lines, desc=\"Processing trigger data\")\n",
    "    for line in bar:\n",
    "        sample_object = json.loads(line.strip())\n",
    "        text = sample_object[\"text\"]\n",
    "        tokens = list(text) # 单字分词\n",
    "        labels = sample_object[\"labels\"]\n",
    "        labels_seq = [\"O\"] * len(tokens) # 默认情况\n",
    "        # 依据 labels 进行标注\n",
    "        for label in labels:\n",
    "            if label[\"trigger\"]:\n",
    "                start = label[\"trigger\"][1] # 起始位置\n",
    "                end = start + len(label[\"trigger\"][0]) # 结束位置\n",
    "                # 触发词标注\n",
    "                labels_seq[start] = \"B-EVENT\"\n",
    "                for i in range(start + 1, end):\n",
    "                    labels_seq[i] = \"I-EVENT\"\n",
    "        # 生成输出\n",
    "        for i in range(len(tokens)):\n",
    "            outlines.append(f\"{tokens[i]} {labels_seq[i]}\")\n",
    "        outlines.append(\"\") # 句子结束标志，用空行分割\n",
    "\n",
    "    ##################\n",
    "\n",
    "    if not os.path.exists(\"./data/processed/trigger\"):\n",
    "        os.makedirs(\"./data/processed/trigger\", exist_ok=True)\n",
    "    with open(f\"./data/processed/trigger/{file}.txt\", \"w\") as f:\n",
    "        f.writelines(\"\\n\".join(outlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf77493",
   "metadata": {},
   "source": [
    "注意我们忽略了 distant_trigger 。\n",
    "\n",
    "同样地来写 process_argument_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_argument_data(file):\n",
    "    \"\"\"\n",
    "    convert raw data into sequence-labeling format data for argument identification, and save to `./data/processed/argument`\n",
    "    event triggers are surrounded by `<event>` `<event/>` markers\n",
    "    each line in the converted contains `token[space]label`\n",
    "    you can use BIO format tagging https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging), or other tagging schema you think fit\n",
    "    use empty line to indicate end of one sentence\n",
    "    if using IOB2 format, the example output would be like\n",
    "    ```\n",
    "    3 B-object\n",
    "    0 I-object\n",
    "    年 I-object\n",
    "    代 I-object\n",
    "    <event> O\n",
    "    参 O\n",
    "    加 O\n",
    "    <event/> O\n",
    "    中 B-subject\n",
    "    共 I-subject\n",
    "    中 I-subject\n",
    "    央 I-subject\n",
    "    的 I-subject\n",
    "    特 I-subject\n",
    "    种 I-subject\n",
    "    领 I-subject\n",
    "    导 I-subject\n",
    "    工 I-subject\n",
    "    作 I-subject\n",
    "\n",
    "    他 O\n",
    "    告 O\n",
    "    诉 O\n",
    "    新 O\n",
    "    京 O\n",
    "    报 O\n",
    "    记 O\n",
    "    者 O\n",
    "    ```\n",
    "    \"\"\"\n",
    "    with open(f\"./data/raw/{file}.json\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    outlines = []\n",
    "    #############\n",
    "    bar = tqdm(lines, desc=\"Processing argument data\")\n",
    "    for line in bar:\n",
    "        sample_object = json.loads(line.strip())\n",
    "        text = sample_object[\"text\"]\n",
    "        tokens = list(text)\n",
    "        labels = sample_object[\"labels\"]\n",
    "        labels_seq = [\"O\"] * len(tokens)\n",
    "        # 依据 labels 进行标注\n",
    "        for label in labels:\n",
    "            # 先处理触发词， 如果强行插入新词的话，不太好，我决定后面输出的时候特别操作。\n",
    "            if label[\"trigger\"]:\n",
    "                start = label[\"trigger\"][1] # 起始位置\n",
    "                end = start + len(label[\"trigger\"][0]) # 结束位置\n",
    "                # 触发词标注\n",
    "                labels_seq[start] = \"B-EVENT\"\n",
    "                for i in range(start + 1, end-1):\n",
    "                    labels_seq[i] = \"O\"\n",
    "                labels_seq[end-1] = \"E-EVENT\" # 触发词最后一个字标注为 E-EVENT\n",
    "\n",
    "            # 处理 object subject\n",
    "            if label[\"object\"]:\n",
    "                start = label[\"object\"][1]\n",
    "                end = start + len(label[\"object\"][0])\n",
    "                labels_seq[start] = \"B-object\"\n",
    "                for i in range(start + 1, end):\n",
    "                    labels_seq[i] = \"I-object\"\n",
    "\n",
    "            if label[\"subject\"]:\n",
    "                start = label[\"subject\"][1]\n",
    "                end = start + len(label[\"subject\"][0])\n",
    "                labels_seq[start] = \"B-subject\"\n",
    "                for i in range(start + 1, end):\n",
    "                    labels_seq[i] = \"I-subject\"\n",
    "\n",
    "        # 生成输出\n",
    "        for i in range(len(tokens)):\n",
    "            if labels_seq[i] == \"B-EVENT\":\n",
    "                outlines.append(\"<event> O\")\n",
    "                outlines.append(f\"{tokens[i]} O\")\n",
    "            elif labels_seq[i] == \"E-EVENT\":\n",
    "                outlines.append(f\"{tokens[i]} O\")\n",
    "                outlines.append(\"<event/> O\")\n",
    "            else:\n",
    "                # 普通情况\n",
    "                outlines.append(f\"{tokens[i]} {labels_seq[i]}\")\n",
    "        outlines.append(\"\") # 句子结束标志，用空行分割\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10f292",
   "metadata": {},
   "source": [
    "难点是 event 标签比较特殊。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40d568",
   "metadata": {},
   "source": [
    "![alt text](image-4.png)\n",
    "\n",
    "这就成功处理。\n",
    "\n",
    "![alt text](image-5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09ce4e",
   "metadata": {},
   "source": [
    "“由于一个事件触发词或论元可能跨越多个 token，因此标注格式采用 `IOB2`” 跨越多个token的意思是多个token组成一个实体，不是说可以跳过中间的token联合后面的token。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "source": [
    "## 数据加载\n",
    "\n",
    "我们看下 utils.py 文件，我们需要完成 read_examples_from_file 和 convert_examples_to_features\n",
    "\n",
    "首先修复语法错误\n",
    "![alt text](image-6.png)\n",
    "\n",
    "改成 f\"{mode:s}-{guid_index:d}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples_from_file(data_dir, mode):\n",
    "    \"\"\"\n",
    "    read file and convert to a list of `InputExample`s\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n",
    "    guid_index = 1\n",
    "    examples = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        file = f.read()\n",
    "    samples = file.split(\"\\n\\n\")\n",
    "    for guid_index, sample in enumerate(samples):\n",
    "        lines = sample.split(\"\\n\")\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            line = line.split(\" \")\n",
    "            if len(line) == 2:\n",
    "                words.append(line[0])\n",
    "                labels.append(line[1])\n",
    "            else: \n",
    "                raise ValueError(\n",
    "                    \"Error in line format: {} in file {}\".format(line, file_path)\n",
    "                )\n",
    "\n",
    "        examples.append(\n",
    "            InputExample(guid=f\"{mode:s}-{guid_index:d}\", words=words, labels=labels)\n",
    "        )\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d015c",
   "metadata": {},
   "source": [
    "原本的逻辑不太方便，我直接重构了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5573b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(\n",
    "    examples: List[InputExample],\n",
    "    label_list,  # a list of all unique labels\n",
    "    max_seq_length: int,  # all sequence should be padded or truncated to `max_seq_length`\n",
    "    tokenizer,  # PretrainedTokenizer\n",
    "    pad_token_label_id: int,  # label id for pad token\n",
    ") -> List[InputFeatures]:  # features\n",
    "    \"\"\"Loads a list of `InputExample`s into a list of `InputFeatures`s\"\"\"\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    pad_token_id = tokenizer.pad_token_id  # padded token id\n",
    "    # tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0]\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for ex_index, example in enumerate(examples):\n",
    "        # Hint: remember to add `[CLS]` and `[SEP]` tokens for BERT model\n",
    "        # e.g. [CLS] the dog is hairy . [SEP]\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "            # Bert模型中，一个单词可能会被切分成多个子词，我们需要将标签分配给这些子词\n",
    "            if len(word_tokens) > 0:\n",
    "                tokens.extend(word_tokens)\n",
    "                # 只有第一个子词保留原始标签，其余子词使用特殊标签 \"X\"\n",
    "                label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # 添加[CLS]和[SEP]标记\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        tokens = [cls_token] + tokens\n",
    "        label_ids = [pad_token_label_id] + label_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # 注意mask的处理，只有真实的token对应的mask值为1，padding的token对应的mask值为0\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        input_mask = input_mask + ([0] * padding_length)\n",
    "        label_ids = label_ids + ([pad_token_label_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, input_mask=input_mask, label_ids=label_ids\n",
    "            )\n",
    "        )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd88d3",
   "metadata": {},
   "source": [
    "## 运行效果\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=7 python -u main.py --mode trigger\n",
    "```\n",
    "\n",
    "![alt text](image-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfb1a3",
   "metadata": {},
   "source": [
    "首先下载了 tokenizer\n",
    "\n",
    "然后开始训练。\n",
    "\n",
    "![alt text](image-8.png)\n",
    "\n",
    "\n",
    "然后我们运行\n",
    "```bash\n",
    "python -u transform.py\n",
    "```\n",
    "\n",
    "![alt text](image-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b25b3",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -u main.py --mode argument\n",
    "```\n",
    "注意学长 README 忘记 --mode 了\n",
    "\n",
    "\n",
    "运行到最后我们才发现，test.txt 里面有 B-time 和 B-location\n",
    "\n",
    "![alt text](image-10.png)\n",
    "\n",
    "![alt text](image-16.png)\n",
    "\n",
    "还遇到了 Max seq length的警告。\n",
    "这是正常的，代码中确实模型设置了 max length 而数据集中确实有“货运司机”的数据，是在句子的末尾，所以无法预测，相应地评估也不评估就行了。\n",
    "\n",
    "![alt text](image-17.png)\n",
    "\n",
    "\n",
    "紧急修复代码 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d806f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if label['time']:\n",
    "    start = label['time'][1]\n",
    "    end = start + len(label['time'][0])\n",
    "    labels_seq[start] = \"B-time\"\n",
    "    for i in range(start + 1, end):\n",
    "        labels_seq[i] = \"I-time\"\n",
    "\n",
    "if label['location']:\n",
    "    start = label['location'][1]\n",
    "    end = start + len(label['location'][0])\n",
    "    labels_seq[start] = \"B-location\"\n",
    "    for i in range(start + 1, end):\n",
    "        labels_seq[i] = \"I-location\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4b85c",
   "metadata": {},
   "source": [
    "![alt text](image-11.png)\n",
    "重新运行。\n",
    "\n",
    "![alt text](image-12.png)\n",
    "一阶段的结果是一样的。\n",
    "\n",
    "再次运行 \n",
    "\n",
    "这一次我们运行成功，可以查看结果\n",
    "\n",
    "![alt text](image-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98b078",
   "metadata": {},
   "source": [
    "注意tokenizer里面自动增加了两个新的token\n",
    "![alt text](image-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1363396",
   "metadata": {},
   "source": [
    "![alt text](image-15.png)\n",
    "\n",
    "预测的输出看起来非常合理。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
