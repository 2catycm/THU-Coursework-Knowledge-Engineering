{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 知识工程-作业4 文本分类\n",
    "\n",
    "2024214500 叶璨铭\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码与文档格式说明\n",
    "\n",
    "> 本文档使用Jupyter Notebook编写，所以同时包括了实验文档和实验代码。\n",
    "\n",
    "> 本次实验项目采用了类似于 Quarto + nbdev 的方法来同步Jupyter Notebook代码到python文件, 因而我们的实验文档导出为pdf和html格式可以进行阅读，而我们的代码也导出为python模块形式，可以作为代码库被其他项目使用。\n",
    "我们这样做的好处是，避免单独管理一堆 .py 文件，防止代码冗余和同步混乱，py文件和pdf文件都是从.ipynb文件导出的，可以保证实验文档和代码的一致性。\n",
    "\n",
    "> 本文档理论上支持多个格式，包括ipynb, html, docx, pdf, md 等，但是由于 quarto和nbdev 系统的一些bug，我们目前暂时只支持ipynb文件，以后有空的时候解决bug可以构建一个[在线文档网站](https://thu-coursework-machine-learning-for-big-data-docs.vercel.app/)。您在阅读本文档时，可以选择您喜欢的格式来进行阅读，建议您使用 Visual Studio Code (或者其他支持jupyter notebook的IDE, 但是VSCode阅读体验最佳) 打开 `ipynb`格式的文档来进行阅读。\n",
    "\n",
    "\n",
    "> 为了记录我们自己修改了哪些地方，使用git进行版本控制，这样可以清晰地看出我们基于助教的代码在哪些位置进行了修改，有些修改是实现了要求的作业功能，而有些代码是对助教的代码进行了重构和优化。我将我在知识工程课程的代码，在作业截止DDL之后，开源到 https://github.com/2catycm/THU-Coursework-Knowledge-Engineering.git ，方便各位同学一起学习讨论。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据下载\n",
    "\n",
    "```bash\n",
    "cd data/raw\n",
    "sh download.sh\n",
    "```\n",
    "\n",
    "但是老师给的链接过期了，\n",
    "\n",
    "\n",
    "![alt text](97966902aa1bdf6a05a0d483a2583da.png)\n",
    "\n",
    "![alt text](9df9b383b00f3511cd9c0f98cdbe9c2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 本次实验使用数据集来自清华大学2016年构建的新闻文本分类数据集\n",
    "THUCNews，共包含14个类别的74万篇新闻文档，可以在\n",
    "http://thuctc.thunlp.org/message 获取，均为UTF-8纯文本格式\n",
    "\n",
    "根据说明，我们进入thunlp的链接，填写研究者信息之后，可以看到下载链接\n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "\n",
    "不过这个数据集特别大，我们这次实验的是子集，所以只用非常好的学长在群里面分享的数据子集，代替脚本中的下载步骤，然后我们就可以解压了。\n",
    "\n",
    "由于文件层级不一样，我们不用脚本，直接解压。\n",
    "\n",
    "![alt text](image-3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "我们看下 main.py 文件\n",
    "\n",
    "使用到数据的地方是\n",
    "\n",
    "```python\n",
    "word2vec_model = load_word2vec_model(file=\"./data/raw/cnews.train.txt\", vector_size=vector_size)\n",
    "...\n",
    "train_dataset = MyDataset(\"./data/raw/cnews.train.txt\", text_vocab=text_vocab, pad_token=pad_token, unk_token=unk_token, max_length=max_length)\n",
    "```\n",
    "\n",
    "\n",
    "load_word2vec_model gensim库会直接处理这个txt，我们稍后再下一节实现\n",
    "\n",
    "实际上训练for循环里面，对于MyDataset的数据要求是这样的\n",
    "\n",
    "```python\n",
    "for text, label in train_loader:\n",
    "    text = text.to(device)\n",
    "    label = label.to(device)\n",
    "    prediction = model(text)\n",
    "    loss = loss_function(prediction, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "所以我们需要去 dataset.py 实现 MyDataset 类，让每一个item是一个text和label的pair\n",
    "\n",
    "我们首先用ruff格式化一下 dataset.py 方便开发 `ruff format dataset.py`\n",
    "\n",
    "注意看，__init__调用了 load 函数需要我们实现\n",
    "\n",
    "```python\n",
    "def __init__(...):\n",
    "    self.text, self.label = self.load(file)\n",
    "```\n",
    "随后检查了这两个数量要一样多，建立了 label2index, word2index, 然后调用了pad。\n",
    "助教用的注释规范太长了，我们使用fastai规范来重新注释。\n",
    "\n",
    "在注释的过程中，我们很快就发现，助教的代码的类型不严谨，self.text 有时候是tensor，有时候是list[list[int]]， 语义不规范，导致VSCode报了很多错，我们先重构一下助教的代码，增加合适的注释和类型提示。\n",
    "\n",
    "清晰的类型注解也是能够帮助我们更好的理解代码的，提高我们对作业的理解，所以不惜花一点时间。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file: str,  # 文件路径\n",
    "        text_vocab: dict,  # 文本词汇\n",
    "        max_length: int = 1024,  # 最大长度\n",
    "        pad_token: str = \"<PAD>\",  # 填充标记\n",
    "        unk_token: str = \"<UNK>\",  # 未知标记\n",
    "        label2index: Optional[dict] = None,  # 标签映射\n",
    "    ) -> None:\n",
    "        # 先写self是更加规范的。\n",
    "        self.text_vocab = text_vocab\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        self.max_length = max_length\n",
    "        # 直接保存的参数写完了，接下来才写计算逻辑\n",
    "\n",
    "        # 加载原始文本和标签\n",
    "        # 这里还没变成张量，不要搞混淆了\n",
    "        raw_text, raw_labels = self.load(file)\n",
    "        assert len(raw_text) == len(raw_labels), \"text: {}, label: {}\".format(\n",
    "            len(raw_text), len(raw_labels)\n",
    "        )\n",
    "        # assert condition, error_message 才是规范写法，助教写print有误。\n",
    "\n",
    "        # 初始化或使用标签映射\n",
    "        if label2index is None:\n",
    "            self.label2index = dict(\n",
    "                zip(sorted(set(raw_labels)), range(len(set(raw_labels))))\n",
    "            )\n",
    "        else:\n",
    "            self.label2index = label2index\n",
    "\n",
    "        # 转换标签为整数\n",
    "        # convert_label2index 函数不应该暴露到外面，而且只有一行，直接在这里实现\n",
    "        self._labels = [self.label2index[label] for label in raw_labels]\n",
    "        assert len(self._labels) == len(raw_labels), \"_labels: {}, raw_labels: {}\".format(\n",
    "            len(self._labels), len(raw_labels)\n",
    "        )\n",
    "\n",
    "        # 转换文本为词索引\n",
    "        indexed_text = self.word2index(raw_text)\n",
    "        assert len(indexed_text) == len(raw_text), \"indexed_text: {}, raw_text: {}\".format(\n",
    "            len(indexed_text), len(raw_text)\n",
    "        )\n",
    "\n",
    "        # 填充并转换为张量\n",
    "        # 合理的接口设计不应该使用 self传递参数，而是应该明确传递。\n",
    "        padded_text = self.pad(indexed_text)\n",
    "        self._text_tensor = torch.tensor(padded_text)\n",
    "    \n",
    "    def __len__(self) -> int:  # 返回数据集大小\n",
    "        return len(self._text_tensor)\n",
    "\n",
    "    def __getitem__(self, item: int  # 数据索引\n",
    "                     ) -> tuple[torch.Tensor, int]:  # 返回(文本张量,标签)\n",
    "        return self._text_tensor[item], self._labels[item]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们严格区分了 _text_tensor 和 raw_text，杜绝了类型问题。\n",
    "\n",
    "现在可以开始按照init中调用的顺序来实现，首先是load函数\n",
    "\n",
    "简单查看一下文件数据，比如cnews.val.txt，\n",
    "\n",
    "![alt text](image-4.png)\n",
    "\n",
    "我们可以看到数据是，类别 \\t 文本 的形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(\n",
    "    self,\n",
    "    file: str,  # 输入文件路径\n",
    ") -> tuple[list[str], list[str]]:  # 返回(文本列表,标签列表)\n",
    "    \"\"\"\n",
    "    read file and load into text (a list of strings) and label (a list of class labels)\n",
    "    \"\"\"\n",
    "    text, label = [], []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # 每行格式: 标签\\t文本内容\n",
    "            label_txt, content = line.strip().split(\"\\t\")\n",
    "            text.append(content)\n",
    "            label.append(label_txt)\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来 word2index \n",
    "\n",
    "这里我们需要把句子的每个单词转换为int\n",
    "\n",
    "cnews.vocab.txt 是这样的\n",
    "\n",
    "![alt text](image-5.png)\n",
    "\n",
    "其实我们没有搞中文分词，直接单字成词，所以要用 .split(\"\") 直接把每个字分开。然后不再vocab里面的要用\\<UNK\\>标注。\n",
    "\n",
    "助教在main中这样写 \n",
    "```python\n",
    "# add unk_token and pad_token\n",
    "unk_index = text_vocab[unk_token] = len(text_vocab)\n",
    "pad_index = text_vocab[pad_token] = len(text_vocab)\n",
    "```\n",
    "实际上数据中已经有 \\<PAD\\> 不过没关系，这是因为助教其实用gensim的word2vec_model.wv.key_to_index作为vocab，而不是原来的那个vocab文件。这个是从训练集提取的。\n",
    "\n",
    "这样搞才是对的，因为待会这里的int tensor还要被word2vec处理为float dense tensor，需要按照人家model的定义来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2index(\n",
    "    self,\n",
    "    text: list[str],  # 输入文本列表\n",
    ") -> list[list[int]]:  # 返回词索引列表的列表\n",
    "    \"\"\"\n",
    "    convert loaded text to word_index with text_vocab\n",
    "    self.text_vocab is a dict\n",
    "    \"\"\"\n",
    "    _text = []\n",
    "    for sentence in text:\n",
    "        # 将句子分词并转换为词索引\n",
    "        words = sentence.strip().split(\"\")\n",
    "        # 如果词不在词表中，使用UNK的索引\n",
    "        indices = [\n",
    "            self.text_vocab.get(word, self.text_vocab[self.unk_token]) for word in words\n",
    "        ]\n",
    "        _text.append(indices)\n",
    "    return _text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在可以写pad，目的是为了让每个句子长度一样，不够的补\\<PAD\\>，太长的截断。\n",
    "\n",
    "用到 self.text_vocab[self.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(\n",
    "    self,\n",
    "    text: list[list[int]],  # 待填充的词索引列表\n",
    ") -> list[list[int]]:  # 返回填充后的词索引列表\n",
    "    \"\"\"\n",
    "    pad word indices to max_length\n",
    "    \"\"\"\n",
    "    pad_text = []\n",
    "    for _text in text:\n",
    "        # 如果长度超过max_length则截断\n",
    "        if len(_text) > self.max_length:\n",
    "            pad_text.append(_text[: self.max_length])\n",
    "        else:\n",
    "            # 如果长度小于max_length则用pad_token的索引填充\n",
    "            pad_text.append(\n",
    "                _text\n",
    "                + [self.text_vocab[self.pad_token]] * (self.max_length - len(_text))\n",
    "            )\n",
    "    return pad_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于gensim工具包训练带有负采样的 skip-gram \n",
    "\n",
    "在本节中，我们将使用gensim工具包来训练一个带有负采样的skip-gram模型。\n",
    "\n",
    "我们首先复习一下课件\n",
    "\n",
    "![alt text](image-1.png)\n",
    "\n",
    "Skip-gram模型是一种用于词向量训练的模型，属于word2vec的一种，通过预测给定词语的上下文词语来学习词向量。负采样是一种加速训练过程的方法，通过减少计算量来提高训练效率。\n",
    "\n",
    "其中课件说的“静态向量”应该是指词向量不参与后续训练。\n",
    "\n",
    "具体学习的原理是，最大化目标词和上下文词的余弦相似度，最小化目标词和负样本词的余弦相似度。\n",
    "负样本太多了，所以从词汇表中采样出来。\n",
    "\n",
    "![alt text](image-2.png)\n",
    "\n",
    "现在我们可以实现代码了。\n",
    "我们首先找到官方仓库的链接，https://github.com/piskvorky/gensim ，根据指南，直接pip 安装即可。readme提到这个库已经是稳定阶段，不再增加新功能。\n",
    "\n",
    "```bash\n",
    "# 安装gensim工具包\n",
    "pip install --upgrade gensim\n",
    "```\n",
    "\n",
    "阅读 main.py 我们可以看到，使用到word2vec的代码如下：\n",
    "\n",
    "```python\n",
    "word2vec_model = load_word2vec_model(file=\"./data/raw/cnews.train.txt\", vector_size=vector_size)\n",
    "word_embeddings = get_word_embeddings(word2vec_model, vector_size=vector_size)\n",
    "```\n",
    "\n",
    "因此我们首先到 `util.py` 实现 load_word2vec_model 函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "import gensim\n",
    "from util import load_text\n",
    "\n",
    "\n",
    "def load_word2vec_model(file=None, vector_size=100):\n",
    "    # train word2vec with gensim\n",
    "    if os.path.exists(\"word2vec\"):\n",
    "        word2vec_model = gensim.models.word2vec.Word2Vec.load(\"word2vec\")\n",
    "    else:\n",
    "        text = load_text(file)\n",
    "        # Train word2vec model with gensim\n",
    "        word2vec_model = gensim.models.word2vec.Word2Vec(\n",
    "            sentences=text, vector_size=vector_size, window=5, min_count=1, workers=4\n",
    "        )\n",
    "        word2vec_model.save(\"word2vec\")\n",
    "    return word2vec_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先if是判断模型是否已经训练成功，如果训练成功，直接加载模型，否则重新训练模型。\n",
    "\n",
    "训练的代码我们参考了文档 https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "的“Training Your Own Model” 章节，使用训练数据 text 作为输入，设置参数 vector_size（向量维度）、窗口大小（window=5）、min_count=1（忽略所有频次小于1的词）和 workers=4（并行训练用的线程数）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN 模型实现\n",
    "\n",
    "在 main.py 里面，\n",
    "```python\n",
    "word_embeddings = get_word_embeddings(word2vec_model, vector_size=vector_size)\n",
    "model = TextCNN(\n",
    "    word_embeddings, vector_size, label2index, pad_index, max_length=1024\n",
    ").to(device)\n",
    "```\n",
    "把 gensim 的 word_embeddings 传入到了 TextCNN的初始化中。\n",
    "\n",
    "这个实际上是一个 np.array 矩阵，在util.py中看到\n",
    "\n",
    "```python\n",
    "def get_word_embeddings(\n",
    "    word2vec_model, vector_size=100, pad_token=\"<PAD>\", unk_token=\"<UNK>\"\n",
    "):\n",
    "    ...\n",
    "    word_embeddings = np.zeros((len(text_vocab), vector_size))\n",
    "    ...\n",
    "    return word_embeddings\n",
    "```\n",
    "\n",
    "每一行是vocab index对应的那个词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以打开 cnn.py，老规矩，先把助教的代码规范化一下，不仅ruff format，还把类型注释搞对，知道每个函数的输入输出和参数的定义和类型。\n",
    "\n",
    "这里代码不多，不需要特别重构。\n",
    "\n",
    "现在我们直接开始写TextCNN.\n",
    "\n",
    "首先我们处理好外面传进来的 word_embeddings ，\n",
    "直接用 https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html nn.Embedding.from_pretrained\n",
    "\n",
    "由于老师讲解强调SGNS是静态嵌入，我们就当做静态嵌入，freeze=True 不参与训练。\n",
    "\n",
    "现在我们看看卷积怎么实现\n",
    "\n",
    "回顾老师课件\n",
    "\n",
    "![alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_size 表示纵向上，认为前后多少个词是有关系的，比如红色框框是2，\n",
    "红色框框对应了卷积核也是那么多，卷起来就是乘法求和，得到单个数。如果一个位置想要得到很多数，那就需要多个卷积核。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随后我们查看 nn.Conv1d 的文档 https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "\n",
    "in_channels 对应 打横的 embedding size， out_channels 对应一个位置输出多少个数， kernel_size 对应纵向的filter_size\n",
    "\n",
    "每一个 fitler_size 卷积完之后，会得到比原来单词数量稍微短一点的向量，为了和位置无关的得到一个全局句子的特征表示，需要做一个pooling，比如课件提到的max pooling。\n",
    "\n",
    "所以说我们分类器的输入有 channels * len(filter_size) 这么多个 （不算batch size，linear只对最后一个维度操作）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        word_embeddings: np.ndarray,  # 预训练词向量矩阵(N*D)\n",
    "        vector_size: int,  # 词向量维度 D\n",
    "        label2index: dict,  # 标签到索引的映射\n",
    "        pad_index: int,  # 填充token的索引\n",
    "        filter_size: list[int] = [2, 3, 4, 5],  # CNN卷积核大小\n",
    "        channels: int = 64,  # CNN输出通道数\n",
    "        max_length: int = 1024,  # 最大序列长度\n",
    "    ) -> None:\n",
    "        super(TextCNN, self).__init__()\n",
    "        # Initialize embedding layer with pre-trained word_embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(word_embeddings), freeze=True, padding_idx=pad_index\n",
    "        )\n",
    "        # Build a stack of 1D CNN layers for each filter size\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(in_channels=vector_size, out_channels=channels, kernel_size=k)\n",
    "                for k in filter_size\n",
    "            ]\n",
    "        )\n",
    "        # Final linear layer for label prediction; number of classes equals len(label2index)\n",
    "        num_class = len(label2index)\n",
    "        self.linear = nn.Linear(channels * len(filter_size), num_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init写好了，forward自然也不难。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "        self,\n",
    "        inputs: torch.Tensor,  # 输入张量(N*L)\n",
    "    ) -> torch.Tensor:  # 返回预测logits(N*K)， 不需要softmax\n",
    "    # Embedding layer\n",
    "    x = self.embedding(inputs)  # 得到 (N*L*D)\n",
    "    # Convolutional layer\n",
    "    x = x.transpose(1, 2)  # 卷积需要将词向量维度放在最后 (N*D*L)\n",
    "    x = [conv(x) for conv in self.convs]\n",
    "    x = [nn.functional.gelu(i) for i in x]  # 每一个 i是 (N*C*Li) ， Li = L - ki + 1\n",
    "    # Pooling layer\n",
    "    x = [\n",
    "        nn.functional.max_pool1d(\n",
    "            i,\n",
    "            kernel_size=i.size(2),  # 对 Li 去做 max_pooling\n",
    "        ).squeeze(2)\n",
    "        for i in x  # 每一个 i是 (N*C*Li)\n",
    "    ]  # 每一个 item 变为 (N*C)\n",
    "    # Concatenate all pooling results\n",
    "    x = torch.cat(x, dim=1)  # 把每一个 item 拼接起来，变为 (N, C*len(filter_size))\n",
    "    # Linear layer\n",
    "    x = self.linear(x)  # 分类，得到 (N*K)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们使用了relu作为激活函数，这个老师没有提到，但是我觉比较需要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查代码，其实不规范的是，init里面 max_length 没有用到，因为前面dataset已经处理过了，不过为了规范，我们还是改一下，forward的时候检查一下。\n",
    "\n",
    "```python\n",
    "# check max_length\n",
    "if inputs.size(1) > self.max_length:\n",
    "    inputs = inputs[:, : self.max_length]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评价指标\n",
    "\n",
    "复习老师课件，macro就是直接每个类别的P，R，f1平均起来，而micro对每个类别的TP，FP，FN求和，然后计算P，R，f1。\n",
    "\n",
    "认为样本量大的时候micro更重要，样本量小的时候macro更重要。\n",
    "\n",
    "注意到 main.py evaluate 类型不够严谨。\n",
    "\n",
    "![alt text](image-8.png)\n",
    "\n",
    "这是因为 sklearn.metrics.precision_recall_fscore_support  , 参考文档 https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "可能返回的不是float，看文档就懂了，没有 average 的时候是各个类别的list，有average的时候是一个数。\n",
    "\n",
    "但是pylance不知道，我们告诉它一定是float就行。\n",
    "\n",
    "```python\n",
    "assert isinstance(micro_f1, float)\n",
    "return micro_f1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "![alt text](image-7.png)\n",
    "\n",
    "原来split写的不好。我们的目的是把str变成char的列表，在Java里面确实经常写 split(\"\")，但是Python里面认为这个不对，所以我们要用python的方式来写。\n",
    "\n",
    "```python\n",
    "words = list(sentence.strip())\n",
    "```\n",
    "这样就行了。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "现在可以成功训练了\n",
    "\n",
    "![alt text](image-9.png)\n",
    "\n",
    "训练成功我们得到了结果\n",
    "\n",
    "![alt text](image-10.png)\n",
    "\n",
    "```bash\n",
    "各类别Precision: [0.996, 0.9752, 0.9624, 0.8462, 0.958, 0.945, 0.9207, 0.9857, 0.9464, 0.9099]\n",
    "各类别Recall: [0.994, 0.982, 0.794, 0.908, 0.89, 0.979, 0.964, 0.968, 0.971, 0.98]\n",
    "各类别F1: [0.995, 0.9786, 0.8701, 0.876, 0.9228, 0.9617, 0.9419, 0.9768, 0.9585, 0.9437]\n",
    "整体微平均Precision: 0.943\n",
    "整体微平均Recall: 0.943\n",
    "整体微平均F1: 0.943\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比老师给出的结果\n",
    "\n",
    "```bash\n",
    "各类别Precision: [0.999, 0.9418, 0.9822, 0.8036, 0.9374, 0.9798, 0.9197, 0.9554, 0.9171, 0.9406]\n",
    "各类别Recall: [0.99, 0.987, 0.719, 0.929, 0.899, 0.972, 0.928, 0.964, 0.973, 0.981]\n",
    "各类别F1: [0.9945, 0.9639, 0.8303, 0.8618, 0.9178, 0.9759, 0.9238, 0.9597, 0.9442, 0.9604]\n",
    "整体微平均Precision: 0.9342\n",
    "整体微平均Recall: 0.9342\n",
    "整体微平均F1: 0.9342\n",
    "```\n",
    "我们的性能提高了1%，有可能是因为我们用了激活函数relu，不知道助教的实现和我们是不是这个区别。\n",
    "\n",
    "参考TextCNN的论文仓库实现 https://github.com/delldu/TextCNN/blob/master/model.py\n",
    "\n",
    "可以看到这个实现里面有relu，比我们多一个dropout。\n",
    "\n",
    "有可能我们数据量少，所以去掉了dropout效果更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探究进一步提高TextCNN性能的思路\n",
    "\n",
    "近期爆火的明星网络KAN，网上褒贬不一，理论上这个网络确实很创新，但是实测效果很多人说视觉领域不一定优于MLP，需要做一些改进才行。\n",
    "\n",
    "我们正好来试试文本分类任务，用卷积KAN代替卷积。\n",
    "\n",
    "其实公式很简单，所谓的KAN就是把矩阵乘法的求和不变，乘法换成了可学习激活函数。\n",
    "\n",
    "![alt text](image-11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们调用开源库 ckan。\n",
    "因为这个作者不太会搞pypi包，弄得有点乱，我用submodule和软链接的方式引入。\n",
    "```bash\n",
    "git submodule add https://github.com/AntonioTepsich/Convolutional-KANs.git\n",
    "cd Convolutional-KANs\n",
    "pip install pyprof\n",
    "cd ..\n",
    "ln -s Convolutional-KANs/kan_convolutional\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于这个库没有搞Conv1d 我们做一个转换器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Conv1dViaConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, \n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True, \n",
    "                 conv_2d=nn.Conv2d):\n",
    "        super(Conv1dViaConv2d, self).__init__()\n",
    "        self.conv2d = conv_2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            (1, kernel_size),\n",
    "            # stride=(1, stride),\n",
    "            padding=(0, padding),\n",
    "            dilation=(1, dilation),\n",
    "            # groups=groups,\n",
    "            # bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 调整输入维度\n",
    "        x = x.unsqueeze(2)  # 添加一个高度维度\n",
    "        # 执行 Conv2d\n",
    "        x = self.conv2d(x)\n",
    "        # 移除多余维度\n",
    "        x = x.squeeze(2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "# 示例用法\n",
    "input_data = torch.randn(1, 3, 10)  # (batch_size, in_channels, length)\n",
    "conv1d_layer = Conv1dViaConv2d(3, 2, 3)\n",
    "output_data = conv1d_layer(input_data)\n",
    "print(output_data.shape)  # 输出：torch.Size([1, 2, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkan_convolutional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mKANConv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KAN_Convolutional_Layer, KAN_Convolution\n\u001b[1;32m      2\u001b[0m conv1d_layer \u001b[38;5;241m=\u001b[39m Conv1dViaConv2d(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, conv_2d\u001b[38;5;241m=\u001b[39mKAN_Convolutional_Layer)\n\u001b[0;32m----> 3\u001b[0m output_data \u001b[38;5;241m=\u001b[39m \u001b[43mconv1d_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_data\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m#\u001b[39;00m\n",
      "File \u001b[0;32m~/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mConv1dViaConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 添加一个高度维度\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 执行 Conv2d\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 移除多余维度\u001b[39;00m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/assignments/THU-Coursework-Knowledge-Engineering/4.新闻文本分类的作业/kan_convolutional/KANConv.py:88\u001b[0m, in \u001b[0;36mKAN_Convolutional_Layer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#if self.n_convs>1:\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiple_convs_kan_conv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/assignments/THU-Coursework-Knowledge-Engineering/4.新闻文本分类的作业/kan_convolutional/convolution.py:50\u001b[0m, in \u001b[0;36mmultiple_convs_kan_conv2d\u001b[0;34m(matrix, kernels, kernel_side, out_channels, stride, dilation, padding, device)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(kern_per_out):\n\u001b[1;32m     49\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m kernels[c_out \u001b[38;5;241m*\u001b[39m kern_per_out \u001b[38;5;241m+\u001b[39m k_idx]\n\u001b[0;32m---> 50\u001b[0m     conv_result \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39mforward(conv_groups[:, k_idx, :, :]\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Apply kernel with non-linear function\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     out_channel_accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m conv_result\u001b[38;5;241m.\u001b[39mview(batch_size, h_out, w_out)\n\u001b[1;32m     53\u001b[0m matrix_out[:, c_out, :, :] \u001b[38;5;241m=\u001b[39m out_channel_accum  \u001b[38;5;66;03m# Store results in output tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/assignments/THU-Coursework-Knowledge-Engineering/4.新闻文本分类的作业/kan_convolutional/convolution.py:50\u001b[0m, in \u001b[0;36mmultiple_convs_kan_conv2d\u001b[0;34m(matrix, kernels, kernel_side, out_channels, stride, dilation, padding, device)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(kern_per_out):\n\u001b[1;32m     49\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m kernels[c_out \u001b[38;5;241m*\u001b[39m kern_per_out \u001b[38;5;241m+\u001b[39m k_idx]\n\u001b[0;32m---> 50\u001b[0m     conv_result \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39mforward(conv_groups[:, k_idx, :, :]\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Apply kernel with non-linear function\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     out_channel_accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m conv_result\u001b[38;5;241m.\u001b[39mview(batch_size, h_out, w_out)\n\u001b[1;32m     53\u001b[0m matrix_out[:, c_out, :, :] \u001b[38;5;241m=\u001b[39m out_channel_accum  \u001b[38;5;66;03m# Store results in output tensor\u001b[39;00m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1698\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:636\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1113\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1091\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:496\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2197\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2194\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2197\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2199\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2202\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2266\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2263\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2264\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2266\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2267\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2269\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/program_files/managers/conda/envs/yuequ/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/program_files/managers/conda/envs/yuequ/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kan_convolutional.KANConv import KAN_Convolutional_Layer, KAN_Convolution\n",
    "conv1d_layer = Conv1dViaConv2d(3, 2, 3, conv_2d=KAN_Convolutional_Layer)\n",
    "output_data = conv1d_layer(input_data)\n",
    "print(output_data.shape)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m kan_conv \u001b[38;5;241m=\u001b[39m KAN_Convolutional_Layer(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      2\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m output_data \u001b[38;5;241m=\u001b[39m \u001b[43mkan_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m output_data\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/program_files/managers/conda/envs/yuequ/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/assignments/THU-Coursework-Knowledge-Engineering/4.新闻文本分类的作业/kan_convolutional/KANConv.py:88\u001b[0m, in \u001b[0;36mKAN_Convolutional_Layer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#if self.n_convs>1:\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiple_convs_kan_conv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/assignments/THU-Coursework-Knowledge-Engineering/4.新闻文本分类的作业/kan_convolutional/convolution.py:50\u001b[0m, in \u001b[0;36mmultiple_convs_kan_conv2d\u001b[0;34m(matrix, kernels, kernel_side, out_channels, stride, dilation, padding, device)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(kern_per_out):\n\u001b[1;32m     49\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m kernels[c_out \u001b[38;5;241m*\u001b[39m kern_per_out \u001b[38;5;241m+\u001b[39m k_idx]\n\u001b[0;32m---> 50\u001b[0m     conv_result \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_groups\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply kernel with non-linear function\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     out_channel_accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m conv_result\u001b[38;5;241m.\u001b[39mview(batch_size, h_out, w_out)\n\u001b[1;32m     53\u001b[0m matrix_out[:, c_out, :, :] \u001b[38;5;241m=\u001b[39m out_channel_accum  \u001b[38;5;66;03m# Store results in output tensor\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/assignments/THU-Coursework-Knowledge-Engineering/4.新闻文本分类的作业/kan_convolutional/KANLinear.py:159\u001b[0m, in \u001b[0;36mKANLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features\n\u001b[1;32m    160\u001b[0m     original_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    161\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kan_conv = KAN_Convolutional_Layer(3, 2, (1, 3))\n",
    "input_data = torch.randn(1, 3, 10, 10)\n",
    "output_data = kan_conv(input_data)\n",
    "output_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看来这个库bug太多了，我们不用它，改换门庭，用 https://github.com/IvanDrokin/torch-conv-kan\n",
    "\n",
    "这个写的好多了，作者明显更加懂Pytorch。直接就有 Conv1d 的实现。\n",
    "\n",
    "这个作者也不懂python打包，我们还是得自己来\n",
    "\n",
    "```bash\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuequ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
