{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 知识工程-作业5 潜在语义索引构建\n",
    "\n",
    "2024214500 叶璨铭\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码与文档格式说明\n",
    "\n",
    "> 本文档使用Jupyter Notebook编写，所以同时包括了实验文档和实验代码。\n",
    "\n",
    "> 本次实验项目采用了类似于 Quarto + nbdev 的方法来同步Jupyter Notebook代码到python文件, 因而我们的实验文档导出为pdf和html格式可以进行阅读，而我们的代码也导出为python模块形式，可以作为代码库被其他项目使用。\n",
    "我们这样做的好处是，避免单独管理一堆 .py 文件，防止代码冗余和同步混乱，py文件和pdf文件都是从.ipynb文件导出的，可以保证实验文档和代码的一致性。\n",
    "\n",
    "> 本文档理论上支持多个格式，包括ipynb, html, docx, pdf, md 等，但是由于 quarto和nbdev 系统的一些bug，我们目前暂时只支持ipynb文件，以后有空的时候解决bug可以构建一个[在线文档网站](https://thu-coursework-machine-learning-for-big-data-docs.vercel.app/)。您在阅读本文档时，可以选择您喜欢的格式来进行阅读，建议您使用 Visual Studio Code (或者其他支持jupyter notebook的IDE, 但是VSCode阅读体验最佳) 打开 `ipynb`格式的文档来进行阅读。\n",
    "\n",
    "\n",
    "> 为了记录我们自己修改了哪些地方，使用git进行版本控制，这样可以清晰地看出我们基于助教的代码在哪些位置进行了修改，有些修改是实现了要求的作业功能，而有些代码是对助教的代码进行了重构和优化。我将我在知识工程课程的代码，在作业截止DDL之后，开源到 https://github.com/2catycm/THU-Coursework-Knowledge-Engineering.git ，方便各位同学一起学习讨论。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码规范说明\n",
    "\n",
    "为了让代码清晰规范，在作业开始前，使用 `ruff format`格式化助教的代码; 在我们实现函数过程中，函数的docstring应当遵循fastai规范而不是numpy规范，这样简洁清晰，不会Repeat yourself。\n",
    "\n",
    "```bash\n",
    "❯ ruff format\n",
    "error: Failed to parse util.py:5:34: Multiple return types must be parenthesized\n",
    "2 files reformatted\n",
    "```\n",
    "\n",
    "原来，老师给的`util.py:5:34`代码这里类型标注不规范，稍作修改即可\n",
    "```python\n",
    "-def load_words(filepath: str) -> List[str], List[List[str]]:\n",
    "+def load_words(filepath: str) -> Tuple[List[str], List[List[str]]]:\n",
    "```\n",
    "\n",
    "此外，原本代码的类型不严谨，语义不规范，导致VSCode报了很多错，我们先重构一下助教的代码，增加合适的注释和类型提示。\n",
    "\n",
    "清晰的类型注解也是能够帮助我们更好的理解代码的，提高我们对作业的理解，所以不惜花一点时间。\n",
    "\n",
    "\n",
    "比如 import * 不是很规范。\n",
    "\n",
    "![alt text](image.png)\n",
    "\n",
    "我们简单修复一下即可\n",
    "\n",
    "```python\n",
    "-from typing import *\n",
    "+from typing import Tuple, List\n",
    "```\n",
    "\n",
    "原本的代码多次错误地使用了 `np.array` 来标注类型，这是不对的，应该使用 `np.ndarray` 来标注类型。比如这个地方\n",
    "\n",
    "```python\n",
    "-def cal_tfidf_matrix(term_doc: np.array, documents: List[List[str]], terms: List[str]):\n",
    "+def cal_tfidf_matrix(term_doc: np.ndarray, documents: List[List[str]], terms: List[str]):\n",
    "```\n",
    "\n",
    "此外这个地方应该用多了, 实际上我们这次作业不需要这个标准库，而且python3.12不建议用。\n",
    "```python\n",
    "-import imp\n",
    "```\n",
    "\n",
    "```bash\n",
    "/home/ye_canming/repos/assignments/THU-Coursework-Knowledge-Engineering/5.潜在语义索引构建的作业/main.py:1: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
    "```\n",
    "\n",
    "\n",
    "这下我们修改完，pylance不再报错了，可以安心写主要逻辑了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理回顾和课件复习\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量空间模型\n",
    "\n",
    "共现矩阵（co-occurrence matrix）有两种，我们这次作业用的是 Term-Document 矩阵，也就是行代表每一个单词，列代表每一个文档，值表示出现次数，文档被这里的列向量表达用来检索，反过来看，每个单词的语义也被行向量所表达。\n",
    "\n",
    "而Term-Term则是上下文同时出现(小窗口，左右4个单词)两个词的次数，从而获得词向量，可以表示单词的相似性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 潜在语义分析\n",
    "\n",
    "用特征来代替词，从数据中学习特征。本来是为了解决同义词不好检索的问题,一义多词(synonymy)。\n",
    "\n",
    "Term Document Matrix A 有 m个单词，n个文档，单词多过文档，rank r\n",
    "\n",
    "$A = U \\sum V^T $, \n",
    "\n",
    "SVD可以截断k，是F范数下的最优近似。\n",
    "\n",
    "#### 潜在语义索引（LSI）\n",
    "\n",
    "就是直接把 U mxr 当做 m个单词的行向量，V rxn 当做 文档的列向量。\n",
    "这就是把文档和单词都变到了 R维的 隐性空间 （主题空间）\n",
    "\n",
    "然后用余弦值作为相似度\n",
    "\n",
    "如果词频矩阵稀疏，SVD计算很难，效果不好。\n",
    "\n",
    "#### 缺点\n",
    "\n",
    "不能解决一词多义(polysemy)，因为单词只有一个向量。\n",
    "\n",
    "SVD是最优 L2 范数近似，但是非负向量的分布不像是高斯分布。\n",
    "\n",
    "忽略词语先后顺序。\n",
    "\n",
    "文档和单词的出现应该是泊松分布？\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 概率LSA\n",
    "文档概率分布 P(d)\n",
    "特定文档的话题分布 P(z|d)\n",
    "特定话题的的单词分布 P(w|z)\n",
    "特定文档的单词分布 P(w|d)\n",
    "\n",
    "![alt text](image-1.png)\n",
    "\n",
    "一个共现矩阵T出现的概率，就是 单词-文档 对出现概率乘上出现次数。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载与结巴分词\n",
    "\n",
    "\n",
    "\n",
    "我们看下 main.py 文件\n",
    "\n",
    "使用到数据的地方是\n",
    "\n",
    "```python\n",
    "terms_tmp, documents_tmp = load_words(os.path.join(filePath, file))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "首先我们确诊一下数据的格式\n",
    "\n",
    "![alt text](image-2.png)\n",
    "从VSCode的警告来看，这个文件未必就是UTF-8，所以filePath 的文本编码不确定，所以我们来个 pip install chardet\n",
    "\n",
    "\n",
    "\n",
    "现在我们来实现这个函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "memory = joblib.Memory(location=\"cache\", verbose=0)\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def load_words(\n",
    "    filepath: str,  # 要加载的文件路径\n",
    ") -> Tuple[List[str], List[List[str]]]:  # (所有词汇列表, 文档分词列表)\n",
    "    \"\"\"读取文件并使用jieba分词\"\"\"\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        # 使用 chardet 检测编码\n",
    "        result = chardet.detect(raw_data)\n",
    "    encoding = result[\"encoding\"]\n",
    "    confidence = result[\"confidence\"]\n",
    "    if confidence < 0.7:\n",
    "        print(\n",
    "            f\"Warning: low confidence ({confidence}) for encoding{encoding}. Using utf-8 instead.\"\n",
    "        )\n",
    "        encoding = \"utf-8\"\n",
    "    print(\n",
    "        f\"Loading file {filepath} with encoding {encoding}, confidence {confidence}. \"\n",
    "    )\n",
    "\n",
    "    with open(filepath, \"r\", encoding=encoding) as f:\n",
    "        documents = []\n",
    "        all_terms = set()\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # 使用jieba分词\n",
    "            words = jieba.lcut(line)\n",
    "            documents.append(words)\n",
    "            all_terms.update(words)\n",
    "\n",
    "    return all_terms, documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中注意jieba用得是 lcut 接口，而不是我们作业1用的tokenize接口。\n",
    "\n",
    "需要注意的是由于外面已经排过序，不需要在这里对terms排序, 只需要正确返回列表即可。\n",
    "\n",
    "为了避免运行太多次，我们使用joblib进行了 cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们这里认为一个句子是一个document！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵构建与TF-IDF\n",
    "\n",
    "首先实现 term_doc = build_term_doc_matrix(documents, terms)·\n",
    "\n",
    "这个没什么特别的，主要是要把索引做对，之所以对term索引而不是对doc索引，是因为 doc是 List[List[str]] 要被遍历。\n",
    "\n",
    "\n",
    "**特别注意这里和理论课课件的方向有所不同**\n",
    "每一行是文档，每一列是单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_term_doc_matrix(\n",
    "    documents: List[List[str]],  # 文档分词列表 (N个文档)\n",
    "    terms: List[str],  # 所有词汇列表 (D个词汇)\n",
    ") -> np.ndarray:  # N×D的矩阵，term_doc[i,j]表示词j在文档i中的出现次数\n",
    "    \"\"\"\n",
    "    build term-document matrix\n",
    "    \"\"\"\n",
    "    # 创建词汇到索引的映射\n",
    "    term_index = {term: idx for idx, term in enumerate(terms)}\n",
    "\n",
    "    # 初始化矩阵\n",
    "    matrix = np.zeros((len(documents), len(terms)), dtype=int)\n",
    "\n",
    "    # 填充矩阵\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        for word in doc:\n",
    "            if word in term_index:\n",
    "                matrix[doc_idx, term_index[word]] += 1\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在写 cal_tfidf_matrix\n",
    "\n",
    "```python\n",
    "TF_IDF = cal_tfidf_matrix(term_doc, documents, terms)\n",
    "    print(\n",
    "        \"TOP10单词的TF-TDF如下所示\\n\",\n",
    "        sorted(TF_IDF.items(), key=lambda x: x[1], reverse=True)[:10],\n",
    "    )\n",
    "```\n",
    "\n",
    "课件上没有强调这个，所以我们补充阅读一下 https://zh.wikipedia.org/zh-hans/Tf-idf\n",
    "\n",
    "所谓 TF，term frequency，就是归一化，对document这个维度，去对term做归一化，让每个document的向量都是L1范数为1的向量。\n",
    "所谓IDF，就是 inverse document frequency，是想说，一个词如果所有文档都有就不重要了。所以就是横向地看在所有文档中，这个单词出现（>0就行）的文档数量，因为要作为重要性，所以1/这个数量。不过至于为什么还有一个 log |D| 我不太明白。\n",
    "\n",
    "所谓 TFIDF，就是TF*IDF。\n",
    "\n",
    "注意如果有单词没有出现，但是在词表里面，那IDF有可能出现定义失败的情况，所以增加参数 epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tfidf_matrix(\n",
    "    term_doc: np.ndarray,  # 词-文档矩阵\n",
    "    documents: List[List[str]],\n",
    "    terms: List[str],\n",
    "    epsilon: float = 1e-6,  # 增加一个很小的数，避免除以0\n",
    ") -> dict:  # TF-IDF值字典 {word: tfidf_value}\n",
    "    \"\"\"\n",
    "    calculate TF-IDF value for each word\n",
    "    \"\"\"\n",
    "    # TF计算\n",
    "    tf = term_doc.astype(float)  # 原本是int\n",
    "    doc_lens = np.sum(tf, axis=1)  # 一共多少个词，在axis1上面求和，消灭axis1\n",
    "    tf = tf / doc_lens  # 归一化\n",
    "\n",
    "    # IDF计算\n",
    "    df = (term_doc > 0).sum(axis=0)  # 每一行是文档，把文档消除，看看单词有出现的次数。\n",
    "    idf = np.log(len(documents) / (df + epsilon))  # 避免除以0\n",
    "\n",
    "    # TF-IDF计算\n",
    "    tfidf = tf * idf\n",
    "\n",
    "    # 转换为字典\n",
    "    return {term: tfidf[:, idx].mean() for idx, term in enumerate(terms)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相似度计算\n",
    "\n",
    "接下来实现 search_key_similarity\n",
    "\n",
    "\n",
    "注意助教代码中，keys = []是从文件名得到的查询关键词，这个没有经过结巴分词，首先我们就需要分词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_key_similarity(\n",
    "    U: np.ndarray,  # SVD分解的U矩阵\n",
    "    s: np.ndarray,  # 奇异值数组\n",
    "    VT: np.ndarray,  # SVD分解的VT矩阵\n",
    "    terms: List[str],  # 词汇列表\n",
    "    term_doc: np.ndarray,\n",
    "    keys: List[str],  # 查询关键词列表\n",
    "    k: int = 10,  # 保留的奇异值数量\n",
    ") -> np.ndarray:  # 相似度矩阵 (N文档, K关键词)\n",
    "    \"\"\"\n",
    "    计算LSI相似度矩阵\n",
    "    \"\"\"\n",
    "    # 构建查询向量\n",
    "    term_index = {term: idx for idx, term in enumerate(terms)}\n",
    "    query_vectors = []\n",
    "\n",
    "    for key in keys:\n",
    "        words = jieba.lcut(key)\n",
    "        vec = np.zeros(len(terms))\n",
    "        for word in words:\n",
    "            if word in term_index:\n",
    "                vec[term_index[word]] += 1\n",
    "        query_vectors.append(vec)\n",
    "\n",
    "    query_matrix = np.array(query_vectors).T  # D x K\n",
    "\n",
    "    # 降维处理\n",
    "    sigma_k = np.diag(s[:k])\n",
    "    U_k = U[:, :k]\n",
    "    VT_k = VT[:k, :]\n",
    "\n",
    "    # 文档在隐空间中的表示\n",
    "    doc_rep = U_k @ sigma_k  # N x k\n",
    "\n",
    "    # 查询在隐空间中的表示\n",
    "    # query_rep = np.linalg.pinv(sigma_k) @ VT_k @ query_matrix  # k x K\n",
    "    sigma_k_inv = np.diag(1.0 / np.diag(sigma_k))\n",
    "    query_rep = sigma_k_inv @ VT_k @ query_matrix\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    return cosine_similarity(doc_rep, query_rep.T)  # 直接用sklearn的余弦相似度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "注意表达式上是-1，但是对角矩阵和正交矩阵不用那么复杂。\n",
    "\n",
    "\n",
    "分类自然也很简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(\n",
    "    sim_matrix: np.ndarray,  # 相似度矩阵 (N文档, K关键词)\n",
    ") -> np.ndarray:  # 预测的类别索引 (N文档,)\n",
    "    \"\"\"\n",
    "    文档分类：为每个文档选择最相似的关键词\n",
    "    \"\"\"\n",
    "    return np.argmax(sim_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_topn_for_each_key(\n",
    "    sim_matrix: np.ndarray,  # 相似度矩阵 (N文档, K关键词)\n",
    "    n: int = 10,  # 保留的Top-N结果数量\n",
    ") -> np.ndarray:  # 搜索结果矩阵 (K关键词, n文档索引)\n",
    "    \"\"\"\n",
    "    为每个关键词搜索Top-N文档\n",
    "    \"\"\"\n",
    "    # 按列排序（每个关键词对应一列）\n",
    "    return np.argsort(-sim_matrix, axis=0)[:n, :].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file ./data/意大利封闭全国.txt with encoding utf-8, confidence 0.99. \n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.878 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading file ./data/戈贝尔确诊新冠.txt with encoding utf-8, confidence 0.99. \n",
      "Loading file ./data/欧洲杯推迟.txt with encoding utf-8, confidence 0.99. \n",
      "Loading file ./data/汤姆汉克斯确诊.txt with encoding utf-8, confidence 0.99. \n",
      "Loading file ./data/特鲁多自我隔离.txt with encoding utf-8, confidence 0.99. \n",
      "Loading file ./data/疫情防控思政大课.txt with encoding utf-8, confidence 0.99. \n",
      "Loading file ./data/美国从阿富汗撤军.txt with encoding utf-8, confidence 0.99. \n",
      "Loading file ./data/英国央行紧急降息.txt with encoding utf-8, confidence 0.99. \n",
      "Loading file ./data/苹果折叠手机专利.txt with encoding utf-8, confidence 0.99. \n",
      "Loading file ./data/魔兽世界怀旧服.txt with encoding utf-8, confidence 0.99. \n",
      "TOP10单词的TF-TDF如下所示\n",
      " [('阿富汗', 0.006327252874692564), ('欧洲杯', 0.005215631936116387), ('折叠', 0.0050477958652063015), ('意大利', 0.0050270258549414346), ('央行', 0.004862996707500968), ('任务', 0.004792787625878285), ('英国', 0.004705536630277911), ('戈贝尔', 0.004662253844871659), ('特鲁多', 0.004567633613773077), ('塔利班', 0.004254171083783253)]\n",
      "U: (652, 652)\n",
      "s: (652,)\n",
      "VT: (23782, 23782)\n",
      "[[0 5 3 2 0]\n",
      " [4 3 1 3 1]\n",
      " [1 2 1 1 1]\n",
      " [4 1 3 3 1]\n",
      " [1 4 3 3 1]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-5: [[ 13 392 203 147  18]\n",
      " [272 203 100 245  91]\n",
      " [100 147  64  90  57]\n",
      " [272 100 203 245 111]\n",
      " [100 272 203 245  90]\n",
      " [392 147  13 132 401]\n",
      " [402 435 403 427 405]\n",
      " [486 482 485 506 468]\n",
      " [520 518 539 540 519]\n",
      " [575 650 637 632 641]]\n",
      "查询结果top-5准确率: [0.4, 0.4, 0.2, 0.4, 0.2, 0.2, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 10\n",
      "分类结果:\n",
      "各类别Precision: [0.8837, 0.0303, 0.7714, 0.3684, 0.1402, 1.0, 1.0, 0.9701, 0.9818, 0.8764]\n",
      "各类别Recall: [0.717, 0.0137, 0.7013, 0.3684, 0.3488, 0.8351, 0.9615, 0.9848, 1.0, 0.975]\n",
      "各类别F1: [0.7917, 0.0189, 0.7347, 0.3684, 0.2, 0.9101, 0.9804, 0.9774, 0.9908, 0.9231]\n",
      "整体微平均Precision: 0.7009\n",
      "整体微平均Recall: 0.7009\n",
      "整体微平均F1: 0.7009\n",
      "[[0 0 0 0 0]\n",
      " [4 1 1 1 1]\n",
      " [2 2 2 2 2]\n",
      " [3 3 3 3 3]\n",
      " [4 4 3 4 3]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-5: [[ 13   2  25  18  23]\n",
      " [272 100  90  64  96]\n",
      " [201 129 143 152 154]\n",
      " [245 206 212 242 203]\n",
      " [272 300 203 262 244]\n",
      " [392 401 313 429 362]\n",
      " [402 431 435 437 403]\n",
      " [482 506 452 474 473]\n",
      " [520 518 539 553 529]\n",
      " [575 620 578 574 650]]\n",
      "查询结果top-5准确率: [1.0, 0.8, 1.0, 1.0, 0.6, 0.6, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 20\n",
      "分类结果:\n",
      "各类别Precision: [0.9412, 0.8272, 0.869, 0.9444, 0.6182, 0.9608, 0.9273, 0.8267, 1.0, 0.8696]\n",
      "各类别Recall: [0.9057, 0.9178, 0.9481, 0.8947, 0.7907, 0.5052, 0.9808, 0.9394, 1.0, 1.0]\n",
      "各类别F1: [0.9231, 0.8701, 0.9068, 0.9189, 0.6939, 0.6622, 0.9533, 0.8794, 1.0, 0.9302]\n",
      "整体微平均Precision: 0.8727\n",
      "整体微平均Recall: 0.8727\n",
      "整体微平均F1: 0.8727\n",
      "[[0 0 0 0 0]\n",
      " [1 1 1 1 1]\n",
      " [2 2 2 2 2]\n",
      " [3 3 3 3 3]\n",
      " [4 4 4 4 4]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-5: [[ 13   2  25  23  19]\n",
      " [100  90  64  56 120]\n",
      " [154 152 137 191 141]\n",
      " [245 206 204 242 212]\n",
      " [272 278 262 266 263]\n",
      " [392 362 132 401 313]\n",
      " [431 437 400 445 414]\n",
      " [482 506 452 474 456]\n",
      " [520 518 539 540 519]\n",
      " [575 574 578 629 624]]\n",
      "查询结果top-5准确率: [1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 30\n",
      "分类结果:\n",
      "各类别Precision: [0.8491, 0.8295, 0.9487, 0.9808, 0.9348, 0.971, 0.8475, 1.0, 0.9474, 0.8989]\n",
      "各类别Recall: [0.8491, 1.0, 0.961, 0.8947, 1.0, 0.6907, 0.9615, 0.9242, 1.0, 1.0]\n",
      "各类别F1: [0.8491, 0.9068, 0.9548, 0.9358, 0.9663, 0.8072, 0.9009, 0.9606, 0.973, 0.9467]\n",
      "整体微平均Precision: 0.9172\n",
      "整体微平均Recall: 0.9172\n",
      "整体微平均F1: 0.9172\n",
      "[[0 0 0 0 0]\n",
      " [3 1 1 1 1]\n",
      " [2 2 2 2 2]\n",
      " [3 3 3 3 3]\n",
      " [4 4 4 4 4]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-5: [[ 13   2  25  32  23]\n",
      " [203 100  56  64  90]\n",
      " [191 152 141 154 137]\n",
      " [245 204 242 259 226]\n",
      " [272 278 266 263 262]\n",
      " [392 401 362 132 429]\n",
      " [431 437 400 414 445]\n",
      " [506 482 456 453 485]\n",
      " [520 539 518 540 553]\n",
      " [575 578 574 629 640]]\n",
      "查询结果top-5准确率: [1.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 40\n",
      "分类结果:\n",
      "各类别Precision: [0.8333, 0.809, 0.8902, 0.9091, 0.8958, 0.9286, 0.8621, 0.9375, 0.9818, 0.9481]\n",
      "各类别Recall: [0.8491, 0.9863, 0.9481, 0.8772, 1.0, 0.6701, 0.9615, 0.9091, 1.0, 0.9125]\n",
      "各类别F1: [0.8411, 0.8889, 0.9182, 0.8929, 0.9451, 0.7784, 0.9091, 0.9231, 0.9908, 0.9299]\n",
      "整体微平均Precision: 0.8972\n",
      "整体微平均Recall: 0.8972\n",
      "整体微平均F1: 0.8972\n",
      "[[0 0 0 0 0]\n",
      " [1 3 1 1 1]\n",
      " [2 2 2 2 2]\n",
      " [3 3 3 3 3]\n",
      " [4 4 4 4 4]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-5: [[ 13   2  25  32   6]\n",
      " [100 203  56  90  64]\n",
      " [152 191 141 154 137]\n",
      " [245 204 259 226 249]\n",
      " [272 278 266 262 263]\n",
      " [392 401 132 362  13]\n",
      " [401 437 431 445 414]\n",
      " [506 456 453 482 485]\n",
      " [539 520 518 553 540]\n",
      " [650 575 646 636 621]]\n",
      "查询结果top-5准确率: [1.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 50\n",
      "分类结果:\n",
      "各类别Precision: [0.8302, 0.7931, 0.8795, 0.8197, 0.9149, 0.9565, 0.9444, 0.9531, 0.9818, 0.9241]\n",
      "各类别Recall: [0.8302, 0.9452, 0.9481, 0.8772, 1.0, 0.6804, 0.9808, 0.9242, 1.0, 0.9125]\n",
      "各类别F1: [0.8302, 0.8625, 0.9125, 0.8475, 0.9556, 0.7952, 0.9623, 0.9385, 0.9908, 0.9182]\n",
      "整体微平均Precision: 0.8957\n",
      "整体微平均Recall: 0.8957\n",
      "整体微平均F1: 0.8957\n",
      "[[0 0 0 0 0]\n",
      " [1 3 1 1 1]\n",
      " [2 2 2 2 2]\n",
      " [3 3 3 3 3]\n",
      " [4 4 4 4 4]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-5: [[ 13  25   2  32  10]\n",
      " [100 203  56 120  96]\n",
      " [152 191 141 154 129]\n",
      " [245 203 249 204 244]\n",
      " [272 266 262 278 281]\n",
      " [392 362 401 132 313]\n",
      " [401 445 400 437 431]\n",
      " [506 482 456 453 486]\n",
      " [539 553 518 520 519]\n",
      " [636 646 650 575 607]]\n",
      "查询结果top-5准确率: [1.0, 0.8, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 100\n",
      "分类结果:\n",
      "各类别Precision: [0.8077, 0.8272, 0.8452, 0.8621, 0.8269, 0.8592, 0.8621, 0.9661, 0.9643, 0.9136]\n",
      "各类别Recall: [0.7925, 0.9178, 0.9221, 0.8772, 1.0, 0.6289, 0.9615, 0.8636, 1.0, 0.925]\n",
      "各类别F1: [0.8, 0.8701, 0.882, 0.8696, 0.9053, 0.7262, 0.9091, 0.912, 0.9818, 0.9193]\n",
      "整体微平均Precision: 0.8727\n",
      "整体微平均Recall: 0.8727\n",
      "整体微平均F1: 0.8727\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了修改参数n查看效果，我们增加argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argpawrse.ArgumentParser()\n",
    "parser.add_argument(\"--n\", type=int, default=5)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用 n=10 试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP10单词的TF-TDF如下所示\n",
      " [('阿富汗', 0.006327252874692564), ('欧洲杯', 0.005215631936116387), ('折叠', 0.0050477958652063015), ('意大利', 0.0050270258549414346), ('央行', 0.004862996707500968), ('任务', 0.004792787625878285), ('英国', 0.004705536630277911), ('戈贝尔', 0.004662253844871659), ('特鲁多', 0.004567633613773077), ('塔利班', 0.004254171083783253)]\n",
      "U: (652, 652)\n",
      "s: (652,)\n",
      "VT: (23782, 23782)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.581 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "[[0 5 3 2 0 6 4 0 2 0]\n",
      " [4 3 1 3 1 4 1 0 1 1]\n",
      " [1 2 1 1 1 1 2 1 3 1]\n",
      " [4 1 3 3 1 1 1 1 1 1]\n",
      " [1 4 3 3 1 1 1 1 1 1]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-10: [[ 13 392 203 147  18 401 272  12 132  52]\n",
      " [272 203 100 245  91 300 111  13  64 116]\n",
      " [100 147  64  90  57  55 132 116 239  86]\n",
      " [272 100 203 245 111  90  64 116  57  56]\n",
      " [100 272 203 245  90  64 116 111  57  55]\n",
      " [392 147  13 132 401 430 203 161 362 313]\n",
      " [402 435 403 427 405 431 401 419 439 423]\n",
      " [486 482 485 506 468 467 452 473 453 474]\n",
      " [520 518 539 540 519 553 529 556 530 546]\n",
      " [575 650 637 632 641 628 645 574 643 611]]\n",
      "查询结果top-10准确率: [0.4, 0.5, 0.2, 0.2, 0.1, 0.3, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 10\n",
      "分类结果:\n",
      "各类别Precision: [0.8837, 0.0303, 0.7714, 0.3684, 0.1402, 1.0, 1.0, 0.9701, 0.9818, 0.8764]\n",
      "各类别Recall: [0.717, 0.0137, 0.7013, 0.3684, 0.3488, 0.8351, 0.9615, 0.9848, 1.0, 0.975]\n",
      "各类别F1: [0.7917, 0.0189, 0.7347, 0.3684, 0.2, 0.9101, 0.9804, 0.9774, 0.9908, 0.9231]\n",
      "整体微平均Precision: 0.7009\n",
      "整体微平均Recall: 0.7009\n",
      "整体微平均F1: 0.7009\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [4 1 1 1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2 2 2 2]\n",
      " [3 3 3 3 3 3 3 3 3 3]\n",
      " [4 4 3 4 3 2 4 1 3 1]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-10: [[ 13   2  25  18  23  47  49  36   6  32]\n",
      " [272 100  90  64  96 120 111  57  56  55]\n",
      " [201 129 143 152 154 191 138 141 137 188]\n",
      " [245 206 212 242 203 204 239 229 259 226]\n",
      " [272 300 203 262 244 132 278 100 245  56]\n",
      " [392 401 313 429 362 132 262  13 303 501]\n",
      " [402 431 435 437 403 423 405 427 433 419]\n",
      " [482 506 452 474 473 486 468 453 467 485]\n",
      " [520 518 539 553 529 519 540 546 556 527]\n",
      " [575 620 578 574 650 638 640 629 642 630]]\n",
      "查询结果top-10准确率: [1.0, 0.9, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 20\n",
      "分类结果:\n",
      "各类别Precision: [0.9412, 0.8272, 0.869, 0.9444, 0.6182, 0.9608, 0.9273, 0.8267, 1.0, 0.8696]\n",
      "各类别Recall: [0.9057, 0.9178, 0.9481, 0.8947, 0.7907, 0.5052, 0.9808, 0.9394, 1.0, 1.0]\n",
      "各类别F1: [0.9231, 0.8701, 0.9068, 0.9189, 0.6939, 0.6622, 0.9533, 0.8794, 1.0, 0.9302]\n",
      "整体微平均Precision: 0.8727\n",
      "整体微平均Recall: 0.8727\n",
      "整体微平均F1: 0.8727\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2 2 2 2]\n",
      " [3 3 3 3 3 3 3 3 3 3]\n",
      " [4 4 4 4 4 4 4 4 4 4]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-10: [[ 13   2  25  23  19  30  32  16  18  14]\n",
      " [100  90  64  56 120 111  96  86  55  57]\n",
      " [154 152 137 191 141 132 143 153 129 163]\n",
      " [245 206 204 242 212 229 259 239 226 238]\n",
      " [272 278 262 266 263 300 264 267 281 282]\n",
      " [392 362 132 401 313 429 305 316 154  47]\n",
      " [431 437 400 445 414 419 420 405 427 401]\n",
      " [482 506 452 474 456 473 467 453 486 468]\n",
      " [520 518 539 540 519 553 546 529 556 527]\n",
      " [575 574 578 629 624 640 630 642 631 641]]\n",
      "查询结果top-10准确率: [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 30\n",
      "分类结果:\n",
      "各类别Precision: [0.8491, 0.8295, 0.9487, 0.9808, 0.9348, 0.971, 0.8475, 1.0, 0.9474, 0.8989]\n",
      "各类别Recall: [0.8491, 1.0, 0.961, 0.8947, 1.0, 0.6907, 0.9615, 0.9242, 1.0, 1.0]\n",
      "各类别F1: [0.8491, 0.9068, 0.9548, 0.9358, 0.9663, 0.8072, 0.9009, 0.9606, 0.973, 0.9467]\n",
      "整体微平均Precision: 0.9172\n",
      "整体微平均Recall: 0.9172\n",
      "整体微平均F1: 0.9172\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [3 1 1 1 1 1 1 1 1 3]\n",
      " [2 2 2 2 2 2 2 2 2 2]\n",
      " [3 3 3 3 3 3 3 3 3 3]\n",
      " [4 4 4 4 4 4 4 4 4 4]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-10: [[ 13   2  25  32  23  17  10  19  14  16]\n",
      " [203 100  56  64  90 120  96 116 111 245]\n",
      " [191 152 141 154 137 129 188 172 153 143]\n",
      " [245 204 242 259 226 229 236 206 249 256]\n",
      " [272 278 266 263 262 281 267 264 300 282]\n",
      " [392 401 362 132 429 313  13 316 305 203]\n",
      " [431 437 400 414 445 401 434 420 428 440]\n",
      " [506 482 456 453 485 474 468 486 473 467]\n",
      " [520 539 518 540 553 519 529 546 527 556]\n",
      " [575 578 574 629 640 650 624 630 632 642]]\n",
      "查询结果top-10准确率: [1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 40\n",
      "分类结果:\n",
      "各类别Precision: [0.8333, 0.809, 0.8902, 0.9091, 0.8958, 0.9286, 0.8621, 0.9375, 0.9818, 0.9481]\n",
      "各类别Recall: [0.8491, 0.9863, 0.9481, 0.8772, 1.0, 0.6701, 0.9615, 0.9091, 1.0, 0.9125]\n",
      "各类别F1: [0.8411, 0.8889, 0.9182, 0.8929, 0.9451, 0.7784, 0.9091, 0.9231, 0.9908, 0.9299]\n",
      "整体微平均Precision: 0.8972\n",
      "整体微平均Recall: 0.8972\n",
      "整体微平均F1: 0.8972\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [1 3 1 1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2 2 2 2]\n",
      " [3 3 3 3 3 3 3 3 3 3]\n",
      " [4 4 4 4 4 4 4 4 4 4]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-10: [[ 13   2  25  32   6  17  30  10  23  14]\n",
      " [100 203  56  90  64 120  86  96 111 116]\n",
      " [152 191 141 154 137 129 188 143 153 172]\n",
      " [245 204 259 226 249 256 236 229 244 242]\n",
      " [272 278 266 262 263 264 281 282 267 300]\n",
      " [392 401 132 362  13 429 313 203 316 154]\n",
      " [401 437 431 445 414 400 420 434 440 409]\n",
      " [506 456 453 482 485 474 486 473 459 468]\n",
      " [539 520 518 553 540 519 529 568 546 559]\n",
      " [650 575 646 636 621 574 629 632 578 640]]\n",
      "查询结果top-10准确率: [1.0, 0.9, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 50\n",
      "分类结果:\n",
      "各类别Precision: [0.8302, 0.7931, 0.8795, 0.8197, 0.9149, 0.9565, 0.9444, 0.9531, 0.9818, 0.9241]\n",
      "各类别Recall: [0.8302, 0.9452, 0.9481, 0.8772, 1.0, 0.6804, 0.9808, 0.9242, 1.0, 0.9125]\n",
      "各类别F1: [0.8302, 0.8625, 0.9125, 0.8475, 0.9556, 0.7952, 0.9623, 0.9385, 0.9908, 0.9182]\n",
      "整体微平均Precision: 0.8957\n",
      "整体微平均Recall: 0.8957\n",
      "整体微平均F1: 0.8957\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [1 3 1 1 1 1 1 1 3 1]\n",
      " [2 2 2 2 2 2 2 2 2 2]\n",
      " [3 3 3 3 3 3 3 3 3 3]\n",
      " [4 4 4 4 4 4 4 4 4 4]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-10: [[ 13  25   2  32  10  17  30   6  14  23]\n",
      " [100 203  56 120  96 111  62 116 245  75]\n",
      " [152 191 141 154 129 137 188 146 143 153]\n",
      " [245 203 249 204 244 236 226 229 256 259]\n",
      " [272 266 262 278 281 282 263 264 302 267]\n",
      " [392 362 401 132 313 316  13 203 262 320]\n",
      " [401 445 400 437 431 420 402 403 414 409]\n",
      " [506 482 456 453 486 485 474 508 468 473]\n",
      " [539 553 518 520 519 568 529 544 532 536]\n",
      " [636 646 650 575 607 626 615 621 629 638]]\n",
      "查询结果top-10准确率: [1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]\n",
      "矩阵K值为: 100\n",
      "分类结果:\n",
      "各类别Precision: [0.8077, 0.8272, 0.8452, 0.8621, 0.8269, 0.8592, 0.8621, 0.9661, 0.9643, 0.9136]\n",
      "各类别Recall: [0.7925, 0.9178, 0.9221, 0.8772, 1.0, 0.6289, 0.9615, 0.8636, 1.0, 0.925]\n",
      "各类别F1: [0.8, 0.8701, 0.882, 0.8696, 0.9053, 0.7262, 0.9091, 0.912, 0.9818, 0.9193]\n",
      "整体微平均Precision: 0.8727\n",
      "整体微平均Recall: 0.8727\n",
      "整体微平均F1: 0.8727\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有652个句子（文档），那n=652 会发生什么呢？那就是把所有的都检索出来了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP10单词的TF-TDF如下所示\n",
      " [('阿富汗', 0.006327252874692564), ('欧洲杯', 0.005215631936116387), ('折叠', 0.0050477958652063015), ('意大利', 0.0050270258549414346), ('央行', 0.004862996707500968), ('任务', 0.004792787625878285), ('英国', 0.004705536630277911), ('戈贝尔', 0.004662253844871659), ('特鲁多', 0.004567633613773077), ('塔利班', 0.004254171083783253)]\n",
      "U: (652, 652)\n",
      "s: (652,)\n",
      "VT: (23782, 23782)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.536 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "[[0 5 3 ... 9 9 9]\n",
      " [4 3 1 ... 7 7 7]\n",
      " [1 2 1 ... 7 6 7]\n",
      " [4 1 3 ... 7 7 7]\n",
      " [1 4 3 ... 7 7 7]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-652: [[ 13 392 203 ... 581 575 650]\n",
      " [272 203 100 ... 482 478 486]\n",
      " [100 147  64 ... 486 414 478]\n",
      " ...\n",
      " [486 482 485 ...  86  64 272]\n",
      " [520 518 539 ... 501  56 203]\n",
      " [575 650 637 ... 401 132 147]]\n",
      "查询结果top-652准确率: [0.08128834355828221, 0.11196319018404909, 0.11809815950920245, 0.08742331288343558, 0.06595092024539877, 0.14877300613496933, 0.07975460122699386, 0.10122699386503067, 0.08282208588957055, 0.12269938650306748]\n",
      "矩阵K值为: 10\n",
      "分类结果:\n",
      "各类别Precision: [0.8837, 0.0303, 0.7714, 0.3684, 0.1402, 1.0, 1.0, 0.9701, 0.9818, 0.8764]\n",
      "各类别Recall: [0.717, 0.0137, 0.7013, 0.3684, 0.3488, 0.8351, 0.9615, 0.9848, 1.0, 0.975]\n",
      "各类别F1: [0.7917, 0.0189, 0.7347, 0.3684, 0.2, 0.9101, 0.9804, 0.9774, 0.9908, 0.9231]\n",
      "整体微平均Precision: 0.7009\n",
      "整体微平均Recall: 0.7009\n",
      "整体微平均F1: 0.7009\n",
      "[[0 0 0 ... 3 4 4]\n",
      " [4 1 1 ... 9 2 0]\n",
      " [2 2 2 ... 9 5 5]\n",
      " [3 3 3 ... 9 6 2]\n",
      " [4 4 3 ... 7 0 7]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-652: [[ 13   2  25 ... 237 300 272]\n",
      " [272 100  90 ... 575 132  32]\n",
      " [201 129 143 ... 648 392 362]\n",
      " ...\n",
      " [482 506 452 ... 203 132 401]\n",
      " [520 518 539 ... 401 132 650]\n",
      " [575 620 578 ... 132 137 203]]\n",
      "查询结果top-652准确率: [0.08128834355828221, 0.11196319018404909, 0.11809815950920245, 0.08742331288343558, 0.06595092024539877, 0.14877300613496933, 0.07975460122699386, 0.10122699386503067, 0.08282208588957055, 0.12269938650306748]\n",
      "矩阵K值为: 20\n",
      "分类结果:\n",
      "各类别Precision: [0.9412, 0.8272, 0.869, 0.9444, 0.6182, 0.9608, 0.9273, 0.8267, 1.0, 0.8696]\n",
      "各类别Recall: [0.9057, 0.9178, 0.9481, 0.8947, 0.7907, 0.5052, 0.9808, 0.9394, 1.0, 1.0]\n",
      "各类别F1: [0.9231, 0.8701, 0.9068, 0.9189, 0.6939, 0.6622, 0.9533, 0.8794, 1.0, 0.9302]\n",
      "整体微平均Precision: 0.8727\n",
      "整体微平均Recall: 0.8727\n",
      "整体微平均F1: 0.8727\n",
      "[[0 0 0 ... 3 4 4]\n",
      " [1 1 1 ... 2 2 2]\n",
      " [2 2 2 ... 1 1 1]\n",
      " [3 3 3 ... 6 4 4]\n",
      " [4 4 4 ... 2 9 2]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-652: [[ 13   2  25 ... 203 300 272]\n",
      " [100  90  64 ... 154 191 132]\n",
      " [154 152 137 ... 115 120  90]\n",
      " ...\n",
      " [482 506 452 ... 137 401 132]\n",
      " [520 518 539 ... 580 636 598]\n",
      " [575 574 578 ... 529 571 567]]\n",
      "查询结果top-652准确率: [0.08128834355828221, 0.11196319018404909, 0.11809815950920245, 0.08742331288343558, 0.06595092024539877, 0.14877300613496933, 0.07975460122699386, 0.10122699386503067, 0.08282208588957055, 0.12269938650306748]\n",
      "矩阵K值为: 30\n",
      "分类结果:\n",
      "各类别Precision: [0.8491, 0.8295, 0.9487, 0.9808, 0.9348, 0.971, 0.8475, 1.0, 0.9474, 0.8989]\n",
      "各类别Recall: [0.8491, 1.0, 0.961, 0.8947, 1.0, 0.6907, 0.9615, 0.9242, 1.0, 1.0]\n",
      "各类别F1: [0.8491, 0.9068, 0.9548, 0.9358, 0.9663, 0.8072, 0.9009, 0.9606, 0.973, 0.9467]\n",
      "整体微平均Precision: 0.9172\n",
      "整体微平均Recall: 0.9172\n",
      "整体微平均F1: 0.9172\n",
      "[[0 0 0 ... 9 4 9]\n",
      " [3 1 1 ... 2 2 2]\n",
      " [2 2 2 ... 1 1 1]\n",
      " [3 3 3 ... 2 9 0]\n",
      " [4 4 4 ... 7 6 2]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-652: [[ 13   2  25 ... 620 272 646]\n",
      " [203 100  56 ... 130 152 191]\n",
      " [191 152 141 ... 100 120  90]\n",
      " ...\n",
      " [506 482 456 ... 170 132 154]\n",
      " [520 539 518 ... 598 648 617]\n",
      " [575 578 574 ... 132  13 130]]\n",
      "查询结果top-652准确率: [0.08128834355828221, 0.11196319018404909, 0.11809815950920245, 0.08742331288343558, 0.06595092024539877, 0.14877300613496933, 0.07975460122699386, 0.10122699386503067, 0.08282208588957055, 0.12269938650306748]\n",
      "矩阵K值为: 40\n",
      "分类结果:\n",
      "各类别Precision: [0.8333, 0.809, 0.8902, 0.9091, 0.8958, 0.9286, 0.8621, 0.9375, 0.9818, 0.9481]\n",
      "各类别Recall: [0.8491, 0.9863, 0.9481, 0.8772, 1.0, 0.6701, 0.9615, 0.9091, 1.0, 0.9125]\n",
      "各类别F1: [0.8411, 0.8889, 0.9182, 0.8929, 0.9451, 0.7784, 0.9091, 0.9231, 0.9908, 0.9299]\n",
      "整体微平均Precision: 0.8972\n",
      "整体微平均Recall: 0.8972\n",
      "整体微平均F1: 0.8972\n",
      "[[0 0 0 ... 3 3 3]\n",
      " [1 3 1 ... 9 2 2]\n",
      " [2 2 2 ... 1 1 1]\n",
      " [3 3 3 ... 2 2 4]\n",
      " [4 4 4 ... 1 6 2]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-652: [[ 13   2  25 ... 243 245 241]\n",
      " [100 203  56 ... 648 130 155]\n",
      " [152 191 141 ...  54 120  90]\n",
      " ...\n",
      " [506 456 453 ... 191 154  47]\n",
      " [539 520 518 ... 580 636 598]\n",
      " [650 575 646 ... 165 130 535]]\n",
      "查询结果top-652准确率: [0.08128834355828221, 0.11196319018404909, 0.11809815950920245, 0.08742331288343558, 0.06595092024539877, 0.14877300613496933, 0.07975460122699386, 0.10122699386503067, 0.08282208588957055, 0.12269938650306748]\n",
      "矩阵K值为: 50\n",
      "分类结果:\n",
      "各类别Precision: [0.8302, 0.7931, 0.8795, 0.8197, 0.9149, 0.9565, 0.9444, 0.9531, 0.9818, 0.9241]\n",
      "各类别Recall: [0.8302, 0.9452, 0.9481, 0.8772, 1.0, 0.6804, 0.9808, 0.9242, 1.0, 0.9125]\n",
      "各类别F1: [0.8302, 0.8625, 0.9125, 0.8475, 0.9556, 0.7952, 0.9623, 0.9385, 0.9908, 0.9182]\n",
      "整体微平均Precision: 0.8957\n",
      "整体微平均Recall: 0.8957\n",
      "整体微平均F1: 0.8957\n",
      "[[0 0 0 ... 4 4 2]\n",
      " [1 3 1 ... 9 0 4]\n",
      " [2 2 2 ... 4 1 1]\n",
      " [3 3 3 ... 1 2 2]\n",
      " [4 4 4 ... 6 2 1]]\n",
      "查询关键词为: ['意大利封闭全国', '戈贝尔确诊新冠', '欧洲杯推迟', '汤姆汉克斯确诊', '特鲁多自我隔离', '疫情防控思政大课', '美国从阿富汗撤军', '英国央行紧急降息', '苹果折叠手机专利', '魔兽世界怀旧服']\n",
      "查询结果top-652: [[ 13  25   2 ... 294 263 168]\n",
      " [100 203  56 ... 623   6 281]\n",
      " [152 191 141 ... 262  86 124]\n",
      " ...\n",
      " [506 482 456 ... 300 153 154]\n",
      " [539 553 518 ... 613 643 636]\n",
      " [636 646 650 ...  97  84 598]]\n",
      "查询结果top-652准确率: [0.08128834355828221, 0.11196319018404909, 0.11809815950920245, 0.08742331288343558, 0.06595092024539877, 0.14877300613496933, 0.07975460122699386, 0.10122699386503067, 0.08282208588957055, 0.12269938650306748]\n",
      "矩阵K值为: 100\n",
      "分类结果:\n",
      "各类别Precision: [0.8077, 0.8272, 0.8452, 0.8621, 0.8269, 0.8592, 0.8621, 0.9661, 0.9643, 0.9136]\n",
      "各类别Recall: [0.7925, 0.9178, 0.9221, 0.8772, 1.0, 0.6289, 0.9615, 0.8636, 1.0, 0.925]\n",
      "各类别F1: [0.8, 0.8701, 0.882, 0.8696, 0.9053, 0.7262, 0.9091, 0.912, 0.9818, 0.9193]\n",
      "整体微平均Precision: 0.8727\n",
      "整体微平均Recall: 0.8727\n",
      "整体微平均F1: 0.8727\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n 652"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询top-n文档准确率与文档分类准确率评估方案的简单对比\n",
    "\n",
    "在本次作业中，我们使用了两种评估方案来衡量模型的性能：查询top-n文档准确率和文档分类准确率。\n",
    "\n",
    "\n",
    "\n",
    "**查询top-n文档准确率**是指对于每个查询关键词，我们从所有文档中选出最相关的n个文档，并计算这些文档中有多少个是与查询关键词相关的。具体来说，我们使用`search_topn_for_each_key`函数来实现这一评估方案。\n",
    "\n",
    "**文档分类准确率**是指将每个文档分配到最相关的查询关键词类别，并计算分类的准确率。具体来说，我们使用`classification`函数来实现这一评估方案。\n",
    "\n",
    "具体来说助教的代码里面有写 label。\n",
    "\n",
    "\n",
    "这两种评估方案肯定是各有优缺点的，适用于不同的应用场景。\n",
    "前者的优点大概是说可以直观地反映出模型在检索任务中的性能，特别是当用户只关心前n个结果时。然而，这种评估方案的缺点是它只考虑了前n个结果，忽略了其他可能相关的文档。\n",
    "\n",
    "后者的优点是可以全面地衡量模型在分类任务中的性能，因为它考虑了所有文档和所有查询关键词。然而，这种评估方案的缺点是它可能会受到类别不平衡的影响，即某些类别的文档数量较少，可能会导致分类结果不准确。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
